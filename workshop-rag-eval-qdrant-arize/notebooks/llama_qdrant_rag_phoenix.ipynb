{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmb4fjQvQmFT"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/qdrant_arize.png\" width=\"500\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tuning a RAG Pipeline using Qdrant and Arize Phoenix</h1>\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Relevant Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "id": "jCyVyMs-JrWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "HYBRID_RAG_PROJECT = \"hybrid-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "NNJI9dP6GeUE"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import ssl\n",
    "import time\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import certifi\n",
    "import nest_asyncio\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, download_loader,\n",
    "    load_index_from_storage, set_global_handler\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator, OpenAIModel, QAEvaluator,\n",
    "    RelevanceEvaluator, run_evals\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnr-p5J6jyEl"
   },
   "source": [
    "### **2. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "zYfgwhXvZcOV",
    "outputId": "3599030d-8ba2-4f8a-c8ec-a0d5b576015b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9n105Mckm1f"
   },
   "source": [
    "Be sure to enable phoenix as your global handler for tracing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "1Ym-djTOkjxQ"
   },
   "outputs": [],
   "source": [
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z15BcuTcj6Cp"
   },
   "source": [
    "### **3. Setup your openai key and retrieve the documents to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EsFBezzcGbG",
    "outputId": "158775a0-8f34-4201-88c5-b221fe40f883"
   },
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieve the documents / dataset to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c3a80fdcb9944e62ba5b51838fef5ef4",
      "af7d2403f4504d49a027d935b13d3f62",
      "79335cc0bf85458ba630ced71d706b2e",
      "43e63c6009c14d9cb4eee3ea177d1c7d",
      "dce488c3561e4d7fa737379553ec0b3b",
      "6ff26418624f4662871a69796f67427a",
      "d5e93af9e5644c419bc653af2057e285",
      "251b1e790043469393aae0c8ebec3d77",
      "602df92e53334c65a2cc171301c67e46",
      "8f997d5f3659421981252a05845dd236",
      "af7527216b9f4240b36024d07ac5c507",
      "02ca3a5d43e343d5b756fca172bc1822",
      "f1e6788ea4344553adbe66202caa656c",
      "5a3d5d83788446028f4de4afbd262f3e",
      "bf06bcbdbff3485abb3895dc54256e00",
      "1ff9ec98314944bdb86d4fda42a4c88b",
      "f9e47bbf438740a2a224c5f7d913e502",
      "c30b902e82de486eb0c10a3e64687b44",
      "4cc3f02b7bb44a87b8d50dd2f98edce7",
      "e07726709c7748eeb708dbe5c37f9770",
      "8115b0b37d164080b58368be3fadbaa0",
      "0d229e81fcb246cf99dd919d06cf5905",
      "eecec2323e294d6fb4ee89f5937c241c",
      "ac507e1d00b446df9379fa58f39418fc",
      "eb848aa889474b118c49a7cdac509d9b",
      "37a8add4c6a242a49ea09ebef29cfdd9",
      "b44f529954e944729b7446a4d610ac1d",
      "c85db136dc2843a58e7aa648bbf9f9fe",
      "5cbb12c59ec948ba8d66dc3812d5c846",
      "adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "90f264886e864ddabdb6eddbecf6d75a",
      "b993aeba0d0f4021b4ae31b8378af903",
      "cfbcd04f19374fe6b8ab9802f6891612"
     ]
    },
    "id": "iisWzELAzYca",
    "outputId": "751b3676-2a8f-4497-de7e-85ee3e9ec199"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"atitaarora/qdrant_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5jYTnnlza6b",
    "outputId": "2894f142-0d41-42b8-b75d-63850ad8a2db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='csv', dataset_name='qdrant_doc', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=1767967, num_examples=240, shard_lengths=None, dataset_name='qdrant_doc')}, download_checksums={'hf://datasets/atitaarora/qdrant_doc@8d859890840f65337c38e96d660b81b1441bbecd/documents.csv': {'num_bytes': 1777260, 'checksum': None}}, download_size=1777260, post_processing_size=None, dataset_size=1767967, size_in_bytes=3545227)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Definition of global chunk properties and chunk processing**\n",
    "Processing each document with desired **TEXT_SPLITTER_ALGO , CHUNK_SIZE , CHUNK_OVERLAP** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global config for chunk processing\n",
    "CHUNK_SIZE = 512 #1000\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Process dataset as langchain (or llamaindex) document for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWTlffFW6Art",
    "outputId": "2685adf0-2965-4c2d-878f-67eee384d65c"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.core import Document\n",
    "\n",
    "## Split and process the document chunks from the given dataset\n",
    "\n",
    "def process_document_chunks(dataset,chunk_size,chunk_overlap):\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(dataset)\n",
    "    ]\n",
    "\n",
    "    # could showcase another variation of processed documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    ## Converting Langchain document chunks above into Llamaindex Document for ingestion\n",
    "    llama_documents = [\n",
    "        Document.from_langchain_format(doc)\n",
    "        for doc in docs_processed\n",
    "    ]\n",
    "    return llama_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbWBhR_661SX",
    "outputId": "79b17993-361d-47e1-ec14-f6f6dfd6949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 14744.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4431"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = process_document_chunks(dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5DrAtMkbL-"
   },
   "source": [
    "### **7. Setting up Qdrant and Collection**\n",
    "\n",
    "We first set up the qdrant client and then create a collection so that our data may be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "QYir5ueWcb8r"
   },
   "outputs": [],
   "source": [
    "##Uncomment to initialise qdrant client in memory\n",
    "#client = qdrant_client.QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "#)\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collection Name \n",
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-IUMQAT76Ep",
    "outputId": "88207dc6-c291-4d76-c8e9-ea6f180ed02a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='qdrant_docs_arize_dense'), CollectionDescription(name='qdrant_docs_arize_hybrid')])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Collection level operations\n",
    "\n",
    "## Get information about existing collections \n",
    "client.get_collections()\n",
    "\n",
    "## Get information about specific collection\n",
    "#collection_info = client.get_collection(COLLECTION_NAME)\n",
    "#print(collection_info)\n",
    "\n",
    "## Deleting collection, if need be\n",
    "#client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring the intended Embedding Model with Fastembed\n",
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "pd.DataFrame(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Document Embedding processing and Ingestion**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and creates a new collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "aa8db2c8310b4f9582f2002adf3ffa1b",
      "b84c10312dbe486fb3b21c7c5e23d603",
      "178bbf06a9ae4b52baa5b731c0c81590",
      "22c94d8ddc9d4561930044518630bb0b",
      "acf3351a0fba41558f97945fbc6b84c7",
      "1f3117c63e8d4558b801a33e9b5c548a",
      "997305a8d871401ba6a1e805dc1ca045",
      "003c1412566441b2a0fd878aabed07a6",
      "cf1c5baa009c467a8c76a1a78d6987b1",
      "18ee1349c9394dab81163d5de23a04a8",
      "21e41bb05ed34b02b2e9e1c2d7d56145",
      "c762c26d4f2e4159b714ad55bbd195eb",
      "b51b292bc0b341de8fd94707e7fa2c0c",
      "f10f2f679d7a4ab8988601cd59aa4b70",
      "d5baee84346c4dc29ffa4fd2fe6495c6",
      "0a8c4c23041641308b625e929790a55e",
      "d8749c8627c44757aa8703b45421af2d",
      "9f88771edbfd468cb315a54b69eb7226",
      "af77bbc36b844afa960381389111bccf",
      "b3dc75c3ec7a48ceaff045c81f250bfd",
      "280b4a0048134a8aaf523deead4e3aef",
      "869c8dd95cf7452b841f7203e0e6b8c5",
      "1ad21fe66fac4742beb7c17ba8f8733f",
      "1677f41c03a74a9f8ac55c49a9c12799",
      "5b8040a4238f41128006185a858db466",
      "7fbfaf28e7134c638b3e4d16e3cb3e6c",
      "1f1e8f2a35b645eb91e96a847cf2856b",
      "bda9ff3e5071497c933f31a2dd44b449",
      "b6686121f3084e1a8c6e0399a8a72675",
      "503e9200b403425b8c0379b0e74b01fe",
      "6f6e8b35568945dc85ba3e47f7b0f3d7",
      "c47e7205eb6540a8a5c51120cd222fe3",
      "ea74a3a2d06a4c61b85db3c7552535a1",
      "fa26918ac71440c1803c82c939fbc79f",
      "366e10ebb46a45508d837c8ead93124c",
      "44cb1b5c1c0a446f9d308d93d33ae4f8",
      "fca056b08fc3427a977bc3717a020443",
      "7e8c241b51af4963aa78d91e6e78136e",
      "6a222b53b0b2448a97b23e2b782a37b5",
      "99f0a6ea567247639e7b17af1e1df9cf",
      "50d006eafd294bbab3339c2bece90673",
      "b21c626648d643d4972c76a094391134",
      "d44a32f577204732a7f4ce767d8e83f5",
      "6b115bdeaeb547e6b96a5a440b2fe3ef"
     ]
    },
    "id": "DTjwoobJcJO3",
    "outputId": "1dcd8c41-4066-4130-f20f-dabc2a1f31d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from phoenix.trace import suppress_tracing\n",
    "## Uncomment it if you'd like to use FastEmbed instead of OpenAI\n",
    "## For the complete list of supported models,\n",
    "##please check https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "##Uncomment if using FastEmbed\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Uncomment it if you'd like to use OpenAI Embeddings instead of FastEmbed\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n",
    "\n",
    "with suppress_tracing():\n",
    "  index = VectorStoreIndex.from_documents(\n",
    "      documents,\n",
    "      storage_context=storage_context,\n",
    "      show_progress=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8a. Connecting to existing Collection**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and uses the previously generated collection to work fully connected with Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment it if using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52PEPokb8FlO",
    "outputId": "1cfa3ea9-09e1-4fdd-eed4-2aa8764ea4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nxhA_6plBSL"
   },
   "source": [
    "### **9.Running an example query and printing out the response.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "id": "Zlep2wVsf1tR"
   },
   "outputs": [],
   "source": [
    "##Initialise retriever to interact with the Qdrant collection\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXE8pRjjgSse",
    "outputId": "6207af91-b168-4a42-c1de-832246c4a001"
   },
   "outputs": [],
   "source": [
    "response = retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(response):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy5llnHLc4lU",
    "outputId": "aa4f5699-9d44-48d9-c24c-fa0cf12e2886"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f7LrVhxuL613",
    "outputId": "397cb8ec-9b3a-44b1-84db-68d75b3bd655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2e4b188e0>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view the above data in the UI\n",
    "px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfOh7f9VlJMX"
   },
   "source": [
    "### **10. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of the baseline questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "01d4ff90aac943ff83bfe49b70c783ba",
      "8d9844dfef3f414aa750e6200a62bfd3",
      "57d8cc8ff7f047c19301f0793367b2ae",
      "4b23a2eb73b6435384686ced47f9c7f6",
      "6fc5bf4070d94eccbdcb4fad4247c758",
      "218ea3901010405a95033a6cb632250a",
      "8ef997a7dbf54d3d9cfa8147e1611cbd",
      "72ce212a2baa494a8a9ae439c7c3e742",
      "f61b254bd9e24480b0176b43c1c7f47e",
      "d9fa5a2efb1142c9abd47fe46f05cee6",
      "56dc39917eec4eba8194bbe0a3e91de1",
      "c95d4cfffece443182faa3e2a3e48dda",
      "df997aad597e4c1ab23501d12383e39b",
      "a1c3867b1d404a948a0edd0e6bc9a1b8",
      "0c90d55a6f6d4d3f8c72bd0ba19687ba",
      "1e846da2f0c44e099d24c350e0b0eb1f",
      "924c8fe0a9cb4feab39d31ec358089b8",
      "cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "ff9f4b2c32234207acb5221babe9c061",
      "d51b6ff6e830489f8e1e100bf1f1ae98",
      "be68eca0c7b640abada072a3cc3cd042",
      "6eadc3109208449eb9cb7ad5b6a5bb46",
      "1057e11184c44ea38154a2ceabd90198",
      "4722676ae36d4749918f5edaac48b182",
      "a2041da6fb5f499191f405fa891c0907",
      "5c26103d1d89466c9d22960bb347db39",
      "0685ef29e4c54e2e8d5c424508ebfe82",
      "ba3888c2a5954fb29293f41577a5bcae",
      "f7b5096a4b944d62a281ee927a1ce2e4",
      "54aa6aff06214fe4baa3e621ec306c04",
      "66588b1f23bb421db7dbc90b2298a845",
      "4ada07ba27b14c1a8c071343cc21c9c4",
      "98d7cd89900f44a79893e7d8d6b8b0c7"
     ]
    },
    "id": "uFLVc1gkivfp",
    "outputId": "f2e8533d-1524-4365-e150-03d4b8ae811f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaa70529d0e48009b3d8d30775e4f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85.8k/85.8k [00:00<00:00, 278kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16299007452e424cb6f7b8572f167e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading the Eval dataset\n",
    "from datasets import load_dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_doc_qna\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obJfFvhLjs86",
    "outputId": "e634866e-4ca5-491b-e457-feda9afa9d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is vaccum optimizer ?',\n",
       " 'Tell me about ‚Äòalways_ram‚Äô parameter?',\n",
       " 'What is difference between scalar and product quantization?',\n",
       " 'What is ‚Äòbest_score‚Äô strategy?',\n",
       " 'How does oversampling helps?',\n",
       " 'What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?',\n",
       " 'What is the purpose of ef_construct in HNSW ?',\n",
       " 'How do you use ‚Äòordering‚Äô parameter?',\n",
       " 'What is significance of ‚Äòon_disk_payload‚Äô setting?',\n",
       " 'What is the impact of ‚Äòwrite_consistency_factor‚Äô ?']"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzQwf65z3yrz",
    "outputId": "225075d8-855e-47d6-cfc6-0f71d2feedd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:59<00:00,  5.91s/it]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      query_engine.query(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDox2bLJlYO1"
   },
   "source": [
    "Check the Phoenix UI as your queries run. Your traces should appear in real time.\n",
    "\n",
    "Open the Phoenix UI with the link below if you haven't already and click through the queries to better understand how the query engine is performing. For each trace you will see a break\n",
    "\n",
    "Phoenix can be used to understand and troubleshoot your by surfacing:\n",
    " - **Application latency** - highlighting slow invocations of LLMs, Retrievers, etc.\n",
    " - **Token Usage** - Displays the breakdown of token usage with LLMs to surface up your most expensive LLM calls\n",
    " - **Runtime Exceptions** - Critical runtime exceptions such as rate-limiting are captured as exception events.\n",
    " - **Retrieved Documents** - view all the documents retrieved during a retriever call and the score and order in which they were returned\n",
    " - **Embeddings** - view the embedding text used for retrieval and the underlying embedding model\n",
    "LLM Parameters - view the parameters used when calling out to an LLM to debug things like temperature and the system prompts\n",
    " - **Prompt Templates** - Figure out what prompt template is used during the prompting step and what variables were used.\n",
    " - **Tool Descriptions** - view the description and function signature of the tools your LLM has been given access to\n",
    " - **LLM Function Calls** - if using OpenAI or other a model with function calls, you can view the function selection and function messages in the input messages to the LLM.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/RAG_trace_details.png\" alt=\"Trace Details View on Phoenix\" style=\"width:100%; height:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "F-fyd_qP31Xf",
    "outputId": "b33bc66c-f2f1-4363-b14a-e96b85e0f5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_M7evWcldWL"
   },
   "source": [
    "### **11. Export and Evaluate Your Trace Data**\n",
    "You can export your trace data as a pandas dataframe for further analysis and evaluation.\n",
    "\n",
    "In this case, we will export our retriever spans into two separate dataframes:\n",
    "\n",
    "queries_df, in which the retrieved documents for each query are concatenated into a single column, retrieved_documents_df, in which each retrieved document is \"exploded\" into its own row to enable the evaluation of each query-document pair in isolation. This will enable us to compute multiple kinds of evaluations, including:\n",
    "\n",
    "relevance: Are the retrieved documents grounded in the response? Q&A correctness: Are your application's responses grounded in the retrieved context? hallucinations: Is your application making up false information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "gQzwROt6JAf7"
   },
   "outputs": [],
   "source": [
    "queries_df = get_qa_with_reference(px.Client())\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>af10580fc044f34b</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The impact of the `write_consistency_factor` is that it determines the number of replicas that are required to acknowledge a write operation before the system responds to the client. If this value is increased, write operations will be more resilient to network partitions within the cluster, as they will be able to tolerate more failures. However, this also means that a greater number of active replicas are needed to successfully perform write operations.</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637b8cb7885e45e8</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored on disk only, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by the service.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd0006eaeda9dbcf</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when making an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the endpoint URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would then include the details of the batch update, specifying the ids, payloads, and vectors for the points you want to update.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80b7acbd5d1a7bb0</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. A larger value for `ef_construct` leads to higher precision in the search but also results in longer indexing time and more space required.</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7a620831aa8c580d</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of `CreatePayloadIndexAsync` is to create an index for a specific field within a collection in a database or search engine. This index is designed to optimize the search and retrieval of data based on the values of the field being indexed. The parameters within the function suggest that it is used to index a text field, with options to specify the tokenizer, minimum and maximum token length, and whether to convert the text to lowercase, which are all relevant for text processing and search optimization.</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9b89ae147db1aa43</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It equalizes the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6e2dca3f21076026</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example and further from a negative one. The strategy selects the best positive and best negative scores for each candidate, and the final score is calculated using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the best negative score squared. This strategy was introduced in version 1.6.0 and its performance scales linearly with the number of examples used.</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01d4da38ee636076</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating-point numbers into 8-bit unsigned integers for each vector component. This method is SIMD-friendly, which can make it faster in computations.\\n\\nProduct quantization, on the other hand, is also used for compressing high-dimensional vectors but involves dividing the vector into smaller sub-vectors and quantizing each of them separately. This method is not SIMD-friendly, which can result in slower distance calculations compared to scalar quantization. Additionally, product quantization typically incurs a loss of accuracy and is recommended for use with high-dimensional vectors.</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6f5f2fe36b1f6b0f</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in certain setups, keeping quantized vectors in RAM can speed up the search process. If you want to store quantized vectors in RAM, you can set `always_ram` to `true`. This parameter can be particularly useful if you experience a significant decrease in search quality and need to ensure faster access to the quantized vectors.</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4c72533de8333fbd</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The term \"vacuum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters, one of which is \"vacuum_min_vector_number.\" This parameter suggests that there is a vacuuming process involved in the optimization, which likely refers to a mechanism for managing and optimizing the storage of vectors. The \"vacuum_min_vector_number\" parameter indicates a threshold for the minimum number of vectors required before the vacuuming process can be triggered. Vacuuming in this context might be related to the process of cleaning up and compacting the data storage to improve performance and efficiency, although the exact nature of the \"vacuum optimizer\" cannot be determined from the given excerpt.</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "af10580fc044f34b           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "637b8cb7885e45e8           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "cd0006eaeda9dbcf                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "80b7acbd5d1a7bb0                What is the purpose of ef_construct in HNSW ?   \n",
       "7a620831aa8c580d            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "9b89ae147db1aa43                                 How does oversampling helps?   \n",
       "6e2dca3f21076026                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "01d4da38ee636076  What is difference between scalar and product quantization?   \n",
       "6f5f2fe36b1f6b0f                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "4c72533de8333fbd                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "af10580fc044f34b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The impact of the `write_consistency_factor` is that it determines the number of replicas that are required to acknowledge a write operation before the system responds to the client. If this value is increased, write operations will be more resilient to network partitions within the cluster, as they will be able to tolerate more failures. However, this also means that a greater number of active replicas are needed to successfully perform write operations.   \n",
       "637b8cb7885e45e8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored on disk only, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by the service.   \n",
       "cd0006eaeda9dbcf  The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when making an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the endpoint URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would then include the details of the batch update, specifying the ids, payloads, and vectors for the points you want to update.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.   \n",
       "80b7acbd5d1a7bb0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. A larger value for `ef_construct` leads to higher precision in the search but also results in longer indexing time and more space required.   \n",
       "7a620831aa8c580d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The purpose of `CreatePayloadIndexAsync` is to create an index for a specific field within a collection in a database or search engine. This index is designed to optimize the search and retrieval of data based on the values of the field being indexed. The parameters within the function suggest that it is used to index a text field, with options to specify the tokenizer, minimum and maximum token length, and whether to convert the text to lowercase, which are all relevant for text processing and search optimization.   \n",
       "9b89ae147db1aa43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It equalizes the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.   \n",
       "6e2dca3f21076026                                                                                                                                                                                                                                                                                                                                                      The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example and further from a negative one. The strategy selects the best positive and best negative scores for each candidate, and the final score is calculated using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the best negative score squared. This strategy was introduced in version 1.6.0 and its performance scales linearly with the number of examples used.   \n",
       "01d4da38ee636076                                                                                                                                                                                                                                                      Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating-point numbers into 8-bit unsigned integers for each vector component. This method is SIMD-friendly, which can make it faster in computations.\\n\\nProduct quantization, on the other hand, is also used for compressing high-dimensional vectors but involves dividing the vector into smaller sub-vectors and quantizing each of them separately. This method is not SIMD-friendly, which can result in slower distance calculations compared to scalar quantization. Additionally, product quantization typically incurs a loss of accuracy and is recommended for use with high-dimensional vectors.   \n",
       "6f5f2fe36b1f6b0f                                                                                                                                                                                                                                                                                                                                                                                                                                                             The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in certain setups, keeping quantized vectors in RAM can speed up the search process. If you want to store quantized vectors in RAM, you can set `always_ram` to `true`. This parameter can be particularly useful if you experience a significant decrease in search quality and need to ensure faster access to the quantized vectors.   \n",
       "4c72533de8333fbd                                                                                                                                                                                                                                  The term \"vacuum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters, one of which is \"vacuum_min_vector_number.\" This parameter suggests that there is a vacuuming process involved in the optimization, which likely refers to a mechanism for managing and optimizing the storage of vectors. The \"vacuum_min_vector_number\" parameter indicates a threshold for the minimum number of vectors required before the vacuuming process can be triggered. Vacuuming in this context might be related to the process of cleaning up and compacting the data storage to improve performance and efficiency, although the exact nature of the \"vacuum optimizer\" cannot be determined from the given excerpt.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "af10580fc044f34b                                                                                                                                                      - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python  \n",
       "637b8cb7885e45e8                                                                                                                * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.  \n",
       "cd0006eaeda9dbcf                                                  - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],  \n",
       "80b7acbd5d1a7bb0                                                                             (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and  \n",
       "7a620831aa8c580d    client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```  \n",
       "9b89ae147db1aa43                                                                                                                                                                                                                                                                                                                                                                                                                                                    ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.  \n",
       "6e2dca3f21076026  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</as...  \n",
       "01d4da38ee636076                                       But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.  \n",
       "6f5f2fe36b1f6b0f  \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...  \n",
       "4c72533de8333fbd                                                                                       return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9b921d98f4d72421</th>\n",
       "      <th>0</th>\n",
       "      <td>7553aa25374ccc3b9ce2042985cb264f</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.832443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7553aa25374ccc3b9ce2042985cb264f</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.825855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9b007f22503fb16c</th>\n",
       "      <th>0</th>\n",
       "      <td>98f3367385a8661f1fb9cb41f67a8397</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>0.821666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98f3367385a8661f1fb9cb41f67a8397</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.787703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2e0bfc0318b338ce</th>\n",
       "      <th>0</th>\n",
       "      <td>9e6e7f4ee9534522b259dbe57b7c242b</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.770652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9e6e7f4ee9534522b259dbe57b7c242b</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "      <td>0.740061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8d75d3042708b6c8</th>\n",
       "      <th>0</th>\n",
       "      <td>3fefcfcbf9d25bae6d695ffbd9b1eecc</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.787150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3fefcfcbf9d25bae6d695ffbd9b1eecc</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "      <td>0.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">b388e45d23bbb452</th>\n",
       "      <th>0</th>\n",
       "      <td>28c99c7776f28de672def56a56604dc6</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.717572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28c99c7776f28de672def56a56604dc6</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "      <td>0.698218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">742ff3b2d7bef55c</th>\n",
       "      <th>0</th>\n",
       "      <td>b276385f6ddeab400f278d552bdbacc7</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.819349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b276385f6ddeab400f278d552bdbacc7</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>0.815198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">95c9e7d276268f2d</th>\n",
       "      <th>0</th>\n",
       "      <td>9fc891451a6ee09f818e9ebee02f4f7a</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.825645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9fc891451a6ee09f818e9ebee02f4f7a</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/aside&gt;</td>\n",
       "      <td>0.808645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">cf253e48ffbb3402</th>\n",
       "      <th>0</th>\n",
       "      <td>5e5c6e4e02f17478354e952573612594</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.857293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e5c6e4e02f17478354e952573612594</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.848822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fbd36fc568a5f7cc</th>\n",
       "      <th>0</th>\n",
       "      <td>3cd4d69e3363a038ddf99071525dc1d4</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.777294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3cd4d69e3363a038ddf99071525dc1d4</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ef4da0b9117e0ef9</th>\n",
       "      <th>0</th>\n",
       "      <td>50b2f2cfca61c0dc414ee9f51425300c</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.718610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50b2f2cfca61c0dc414ee9f51425300c</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "      <td>0.707060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "9b921d98f4d72421 0                  7553aa25374ccc3b9ce2042985cb264f   \n",
       "                 1                  7553aa25374ccc3b9ce2042985cb264f   \n",
       "9b007f22503fb16c 0                  98f3367385a8661f1fb9cb41f67a8397   \n",
       "                 1                  98f3367385a8661f1fb9cb41f67a8397   \n",
       "2e0bfc0318b338ce 0                  9e6e7f4ee9534522b259dbe57b7c242b   \n",
       "                 1                  9e6e7f4ee9534522b259dbe57b7c242b   \n",
       "8d75d3042708b6c8 0                  3fefcfcbf9d25bae6d695ffbd9b1eecc   \n",
       "                 1                  3fefcfcbf9d25bae6d695ffbd9b1eecc   \n",
       "b388e45d23bbb452 0                  28c99c7776f28de672def56a56604dc6   \n",
       "                 1                  28c99c7776f28de672def56a56604dc6   \n",
       "742ff3b2d7bef55c 0                  b276385f6ddeab400f278d552bdbacc7   \n",
       "                 1                  b276385f6ddeab400f278d552bdbacc7   \n",
       "95c9e7d276268f2d 0                  9fc891451a6ee09f818e9ebee02f4f7a   \n",
       "                 1                  9fc891451a6ee09f818e9ebee02f4f7a   \n",
       "cf253e48ffbb3402 0                  5e5c6e4e02f17478354e952573612594   \n",
       "                 1                  5e5c6e4e02f17478354e952573612594   \n",
       "fbd36fc568a5f7cc 0                  3cd4d69e3363a038ddf99071525dc1d4   \n",
       "                 1                  3cd4d69e3363a038ddf99071525dc1d4   \n",
       "ef4da0b9117e0ef9 0                  50b2f2cfca61c0dc414ee9f51425300c   \n",
       "                 1                  50b2f2cfca61c0dc414ee9f51425300c   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "9b921d98f4d72421 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "9b007f22503fb16c 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "2e0bfc0318b338ce 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "8d75d3042708b6c8 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "b388e45d23bbb452 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "742ff3b2d7bef55c 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "95c9e7d276268f2d 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "cf253e48ffbb3402 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "fbd36fc568a5f7cc 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "ef4da0b9117e0ef9 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "9b921d98f4d72421 0                                                                                                                                                                                                                                                       - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "                 1                                  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "9b007f22503fb16c 0                                                                                                  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                                 The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "2e0bfc0318b338ce 0                                                                                                                                                                   - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "                 1                  ```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],   \n",
       "8d75d3042708b6c8 0                                                                  (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "                 1                                                                                                                                              The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and   \n",
       "b388e45d23bbb452 0                                                                                   client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "                 1                                                    },\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```   \n",
       "742ff3b2d7bef55c 0                                                                                                                                                            ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 1                                                                                                                                                                                                                                                                                                                                                                                                                           oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "95c9e7d276268f2d 0                                                                                  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "                 1                                                  The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</aside>   \n",
       "cf253e48ffbb3402 0                                                                                           But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "                 1                                                                               *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "fbd36fc568a5f7cc 0                                                                       \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "                 1                                                          It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "ef4da0b9117e0ef9 0                                                                                                                                                                 return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "                 1                                                         },\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "9b921d98f4d72421 0                        0.832443  \n",
       "                 1                        0.825855  \n",
       "9b007f22503fb16c 0                        0.821666  \n",
       "                 1                        0.787703  \n",
       "2e0bfc0318b338ce 0                        0.770652  \n",
       "                 1                        0.740061  \n",
       "8d75d3042708b6c8 0                        0.787150  \n",
       "                 1                        0.767757  \n",
       "b388e45d23bbb452 0                        0.717572  \n",
       "                 1                        0.698218  \n",
       "742ff3b2d7bef55c 0                        0.819349  \n",
       "                 1                        0.815198  \n",
       "95c9e7d276268f2d 0                        0.825645  \n",
       "                 1                        0.808645  \n",
       "cf253e48ffbb3402 0                        0.857293  \n",
       "                 1                        0.848822  \n",
       "fbd36fc568a5f7cc 0                        0.777294  \n",
       "                 1                        0.765625  \n",
       "ef4da0b9117e0ef9 0                        0.718610  \n",
       "                 1                        0.707060  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Define your evaluation model and your evaluators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4XwhUYRlkdP"
   },
   "source": [
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7b8b3e1c571a4345bc2ec4eb00f0967e",
      "a74d6dce715f4494874de92852b56c09",
      "a5d80d3579ca481a98310bd08b8c6918",
      "f07a59b5bc7a4f56baa892ca864d488c",
      "84fd76a737334831a886755affb50886",
      "5c65981c04f34589a36c5c8bb4195f2b",
      "10d5d30b4fd14f3ba45da1794607c5e0",
      "ce0efd7352cd4539bda8706ebbc146fd",
      "128b89e88c684401818304e8410b6c33",
      "e7d4e009f37a4d798dbc23b39ed7c0b1",
      "d772ecb46ca54adbbb84749fe07ab735",
      "68ac22f645934827a9627e3559067ba1",
      "f3c5e97c2ac94cafa20b130f1f45ce67",
      "b505d5e7978d4b19b59dc8f3fe71e795",
      "080a6d0c05144698a9c7366e54bb39b0",
      "e69da775e4df4ed8a5f5ed1348278ab8",
      "77d79ac84c8c4d8393fcdb7f304b4bf3",
      "43dfd4070e324858974c08fbcb8055fd",
      "38d701e482684695b97bd73bac78ccfc",
      "e7d21aa7bce145b78e834897b158f9b0",
      "65f86e93b0fe48ddbaf2cabc9975c68b",
      "8e261a6071cd41a6afd4377c9ead98ca"
     ]
    },
    "id": "SnJSWjOkJGpE",
    "outputId": "b7119003-dd01-4a37-da4f-ed2bf3f130d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82484f71bbf4f97a8af4284578f181f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc28eef873248108a008f180db21d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOWquDbAlm4P"
   },
   "source": [
    "Your evaluations should now appear as annotations on the appropriate spans in Phoenix.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5yek2mtlrRA"
   },
   "source": [
    "### **13. Let's try Hybrid search now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a new collection to store your hybrid emebeddings\n",
    "COLLECTION_NAME_HYBRID = \"qdrant_docs_arize_hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reprocess documents with different settings if needed \n",
    "#documents = process_document_chunks(dataset , CHUNK_SIZE , CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n",
       " {'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##List of supported sparse vector models\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding\n",
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Ingest Sparse and Dense vectors into Qdrant**\n",
    "\n",
    "Ingest sparse and dense vectors into Qdrant Collection.\n",
    "We are using Splade++ model for Sparse Vector Model and default Fastembed model - bge-small-en-1.5 for dense embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1203f65ae2084a8a8f48e76bbab8d4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fd7029ba31435b9e77285781e58a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eca322faff430e98ca0eda0e136671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a203f651a55a462f8bb5f65491b66372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2871470268094878a20b965de14a9d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718e93012d34b5190317a6faffad5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding, SparseEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from typing import List, Tuple\n",
    "\n",
    "sparse_model_name = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "\n",
    "batch_size = 10\n",
    "parallel = 0\n",
    "\n",
    "## Computing sparse vectors\n",
    "def compute_sparse_vectors(\n",
    "    texts: List[str],\n",
    "    ) -> Tuple[List[List[int]], List[List[float]]]:\n",
    "    indices, values = [], []\n",
    "    for embedding in sparse_model.embed(texts):\n",
    "        indices.append(embedding.indices.tolist())\n",
    "        values.append(embedding.values.tolist())\n",
    "    return indices, values\n",
    "\n",
    "## Creating a vector store with Hybrid search enabled\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    sparse_doc_fn=compute_sparse_vectors,\n",
    "    sparse_query_fn=compute_sparse_vectors)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Ingesting sparse and dense vectors into Qdrant collection\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=8862, indexed_vectors_count=4429, points_count=4431, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors={'text-dense': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'text-sparse': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None))}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collection level operations\n",
    "client.get_collection(COLLECTION_NAME_HYBRID)\n",
    "#client.delete_collection(COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the number of documents matches the expected number of document chunks \n",
    "client.count(collection_name=COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. Hybrid Search with Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Like many other databases, Qdrant does not delete entries immediately after a query.\n",
      "\n",
      "Instead, it marks records as deleted and ignores them for future queries.\n",
      "\n",
      "\n",
      "\n",
      "This strategy allows us to minimize disk access - one of the slowest operations.\n",
      "\n",
      "However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n",
      "\n",
      "\n",
      "\n",
      "To avoid these adverse effects, Vacuum Optimizer is used.\n",
      "\n",
      "It is used if the segment has accumulated too many deleted records.\n",
      "\n",
      "2 In this case, the segment to be optimized remains readable for the time of the rebuild.\n",
      "\n",
      "\n",
      "\n",
      "![Segment optimization](/docs/optimization.svg)\n",
      "\n",
      "\n",
      "\n",
      "The availability is achieved by wrapping the segment into a proxy that transparently handles data changes.\n",
      "\n",
      "Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.\n",
      "\n",
      "\n",
      "\n",
      "## Vacuum Optimizer\n",
      "\n",
      "\n",
      "\n",
      "The simplest example of a case where you need to rebuild a segment repository is to remove points.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Before trying Hybrid search , lets try Sparse Vector Search Retriever \n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "sparse_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.SPARSE,\n",
    "    sparse_top_k=2,\n",
    ")\n",
    "\n",
    "## Pure sparse vector search\n",
    "nodes = sparse_retriever.retrieve(\"What is a Vacuum Optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try Hybrid Search Retriever now using the 'alpha' parameter that controls the weightage between\n",
    "## the dense and sparse vector search scores.\n",
    "\n",
    "hybrid_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    sparse_top_k=2,\n",
    "    similarity_top_k=5,\n",
    "    alpha=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Such segments, for example, are created as copy-on-write segments during optimization itself.\n",
      "\n",
      "\n",
      "\n",
      "It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n",
      "\n",
      "On the other hand, too many small segments lead to suboptimal search performance.\n",
      "\n",
      "\n",
      "\n",
      "There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\n",
      "2 ---\n",
      "\n",
      "title: Optimizer\n",
      "\n",
      "weight: 70\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../optimizer\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Optimizer\n",
      "\n",
      "\n",
      "\n",
      "It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n",
      "\n",
      "\n",
      "\n",
      "Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n",
      "3 The criteria for starting the optimizer are defined in the configuration file.\n",
      "\n",
      "\n",
      "\n",
      "Here is an example of parameter values:\n",
      "\n",
      "\n",
      "\n",
      "```yaml\n",
      "\n",
      "storage:\n",
      "\n",
      "  optimizers:\n",
      "\n",
      "    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.\n",
      "\n",
      "    max_segment_number: 5\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## Indexing Optimizer\n",
      "\n",
      "\n",
      "\n",
      "Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.\n",
      "4 size: 768,\n",
      "\n",
      "                distance: Distance::Cosine.into(),\n",
      "\n",
      "                ..Default::default()\n",
      "\n",
      "            })),\n",
      "\n",
      "        }),\n",
      "\n",
      "        optimizers_config: Some(OptimizersConfigDiff {\n",
      "\n",
      "            default_segment_number: Some(16),\n",
      "\n",
      "            ..Default::default()\n",
      "\n",
      "        }),\n",
      "\n",
      "        ..Default::default()\n",
      "\n",
      "    })\n",
      "\n",
      "    .await?;\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "```java\n",
      "\n",
      "import io.qdrant.client.QdrantClient;\n",
      "\n",
      "import io.qdrant.client.QdrantGrpcClient;\n",
      "\n",
      "import io.qdrant.client.grpc.Collections.CreateCollection;\n",
      "5 optimizersConfig: new OptimizersConfigDiff { DefaultSegmentNumber = 2 }\n",
      "\n",
      ");\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "## Let's try hybrid retriever \n",
    "nodes = hybrid_retriever.retrieve(\"What is merge optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't be modifying the alpha parameter after the retriever has been created\n",
    "# but that's the easiest way to show the effect of the parameter\n",
    "#hybrid_retriever._alpha = 0.1\n",
    "#hybrid_retriever._alpha = 0.9\n",
    "\n",
    "#nodes = hybrid_retriever.retrieve(\"What are the advantages of quantization?\")\n",
    "#for i, node in enumerate(nodes):\n",
    "#    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/run-llama/llama_index/blob/6af5b1377a652f3e1de2f8523e0cbab2378ebb33/llama-index-integrations/vector_stores/llama-index-vector-stores-qdrant/llama_index/vector_stores/qdrant/utils.py#L96-L102\n",
    "## For the purpose of showing how to customize the score distribution in Hybrid search\n",
    "\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryResult\n",
    "\n",
    "def relative_score_custom_fusion(\n",
    "    dense_result: VectorStoreQueryResult,\n",
    "    sparse_result: VectorStoreQueryResult,\n",
    "    # NOTE: only for hybrid search (0 for sparse search, 1 for dense search)\n",
    "    alpha: float = 0.1,\n",
    "    top_k: int = 2,\n",
    ") -> VectorStoreQueryResult:\n",
    "    \"\"\"\n",
    "    Fuse dense and sparse results using relative score fusion.\n",
    "    \"\"\"\n",
    "    # check if dense or sparse results is empty\n",
    "    if (dense_result.nodes is None or len(dense_result.nodes) == 0) and (\n",
    "        sparse_result.nodes is None or len(sparse_result.nodes) == 0\n",
    "    ):\n",
    "        return VectorStoreQueryResult(nodes=None, similarities=None, ids=None)\n",
    "    elif sparse_result.nodes is None or len(sparse_result.nodes) == 0:\n",
    "        return dense_result\n",
    "    elif dense_result.nodes is None or len(dense_result.nodes) == 0:\n",
    "        return sparse_result\n",
    "\n",
    "    assert dense_result.nodes is not None\n",
    "    assert dense_result.similarities is not None\n",
    "    assert sparse_result.nodes is not None\n",
    "    assert sparse_result.similarities is not None\n",
    "\n",
    "    # deconstruct results\n",
    "    sparse_result_tuples = list(zip(sparse_result.similarities, sparse_result.nodes))\n",
    "    sparse_result_tuples.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    dense_result_tuples = list(zip(dense_result.similarities, dense_result.nodes))\n",
    "    dense_result_tuples.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # track nodes in both results\n",
    "    all_nodes_dict = {x.node_id: x for x in dense_result.nodes}\n",
    "    for node in sparse_result.nodes:\n",
    "        if node.node_id not in all_nodes_dict:\n",
    "            all_nodes_dict[node.node_id] = node\n",
    "\n",
    "    # normalize sparse similarities from 0 to 1\n",
    "    sparse_similarities = [x[0] for x in sparse_result_tuples]\n",
    "\n",
    "    sparse_per_node = {}\n",
    "    if len(sparse_similarities) > 0:\n",
    "        max_sparse_sim = max(sparse_similarities)\n",
    "        min_sparse_sim = min(sparse_similarities)\n",
    "\n",
    "        # avoid division by zero\n",
    "        if max_sparse_sim == min_sparse_sim:\n",
    "            sparse_similarities = [max_sparse_sim] * len(sparse_similarities)\n",
    "        else:\n",
    "            sparse_similarities = [\n",
    "                (x - min_sparse_sim) / (max_sparse_sim - min_sparse_sim)\n",
    "                for x in sparse_similarities\n",
    "            ]\n",
    "\n",
    "        sparse_per_node = {\n",
    "            sparse_result_tuples[i][1].node_id: x\n",
    "            for i, x in enumerate(sparse_similarities)\n",
    "        }\n",
    "\n",
    "    # normalize dense similarities from 0 to 1\n",
    "    dense_similarities = [x[0] for x in dense_result_tuples]\n",
    "\n",
    "    dense_per_node = {}\n",
    "    if len(dense_similarities) > 0:\n",
    "        max_dense_sim = max(dense_similarities)\n",
    "        min_dense_sim = min(dense_similarities)\n",
    "\n",
    "        # avoid division by zero\n",
    "        if max_dense_sim == min_dense_sim:\n",
    "            dense_similarities = [max_dense_sim] * len(dense_similarities)\n",
    "        else:\n",
    "            dense_similarities = [\n",
    "                (x - min_dense_sim) / (max_dense_sim - min_dense_sim)\n",
    "                for x in dense_similarities\n",
    "            ]\n",
    "\n",
    "        dense_per_node = {\n",
    "            dense_result_tuples[i][1].node_id: x\n",
    "            for i, x in enumerate(dense_similarities)\n",
    "        }\n",
    "\n",
    "    # fuse the scores\n",
    "    fused_similarities = []\n",
    "    for node_id in all_nodes_dict:\n",
    "        sparse_sim = sparse_per_node.get(node_id, 0)\n",
    "        dense_sim = dense_per_node.get(node_id, 0)\n",
    "        fused_sim = (1 - alpha) * sparse_sim + alpha * dense_sim\n",
    "        fused_similarities.append((fused_sim, all_nodes_dict[node_id]))\n",
    "\n",
    "    fused_similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    fused_similarities = fused_similarities[:top_k]\n",
    "\n",
    "    # create final response object\n",
    "    return VectorStoreQueryResult(\n",
    "        nodes=[x[1] for x in fused_similarities],\n",
    "        similarities=[x[0] for x in fused_similarities],\n",
    "        ids=[x[1].node_id for x in fused_similarities],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Initialise Hybrid Vector Store \n",
    "vector_store_hybrid = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,  # This is important for the ingestion\n",
    "    hybrid_fusion_fn = relative_score_custom_fusion,\n",
    ")\n",
    "\n",
    "## Followed by initializing index for interacting with the Hybrid Collection in Qdrant\n",
    "\n",
    "hybrid_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store_hybrid,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Re-Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "Let's rerun the list of the baseline questions about Qdrant on the Hybrid Retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                        | 1/10 [00:08<01:13,  8.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                           | 2/10 [00:13<00:51,  6.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                             | 3/10 [00:21<00:51,  7.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                | 4/10 [00:27<00:39,  6.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 5/10 [00:31<00:28,  5.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 6/10 [00:38<00:25,  6.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 7/10 [00:41<00:15,  5.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 8/10 [00:56<00:16,  8.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 9/10 [01:00<00:06,  6.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:06<00:00,  6.67s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "# All spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "\n",
    "    ##Reuse the previously loaded dataset `qdrant_qa_question`\n",
    "    query_engine_hybrid = hybrid_index.as_query_engine()\n",
    "    for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "        try:\n",
    "          query_engine_hybrid.query(query)\n",
    "        except Exception as e:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "\n",
    "queries_df_hybrid = get_qa_with_reference(px.Client(), project_name=HYBRID_RAG_PROJECT)\n",
    "retrieved_documents_df_hybrid = get_retrieved_documents(px.Client(), project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212d2c6b46f2474b</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The impact of the `write_consistency_factor` is that it determines the number of replicas in a distributed system that must acknowledge a write operation before the system responds to the client. If the `write_consistency_factor` is increased, write operations will be more tolerant to network partitions, as more replicas are required to acknowledge the write. However, this also means that a higher number of active replicas are needed to successfully perform write operations. If not enough replicas are available to meet the `write_consistency_factor`, write operations will not be completed, potentially affecting the availability of the system for writing data.</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5b680cfae0d4b4f1</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0aa7f26ef2ea7615</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when sending an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their IDs, payloads, and vectors.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until it has received a response from the leader replica. This ensures that all replicas process the operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e3d1118a55429921</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. A larger value for `ef_construct` leads to higher precision in the search but also results in longer indexing time.</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acad6907c9d13ff9</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of the `CreatePayloadIndexAsync` method is to create an index for a specific field within a collection in a database. This index is designed to optimize the search and retrieval of data based on the values of that field. The parameters within the method specify the name of the field to be indexed, the schema of the field including its type (e.g., text), the tokenizer to be used (e.g., word), the minimum and maximum token length, and whether the text should be converted to lowercase. This asynchronous operation facilitates efficient querying of text fields by structuring the index to quickly locate entries based on the indexed field's criteria.</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>048a6afc49aa9229</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps in two distinct ways. In the context of quantization for similarity search algorithms, it allows for significant compression of high-dimensional vectors in memory while compensating for accuracy loss by re-scoring additional points with the original vectors. In the context of training datasets, oversampling helps to equalize the representation of classes, enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861ecba1ca97b94</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example. It selects the best positive and best negative scores from these comparisons. The final score for a candidate is determined using a step formula: if the best positive score is greater than the best negative score, the final score is the best positive score; otherwise, the final score is the negative of the best negative score squared. This strategy was introduced in version 1.6.0 and its performance scales linearly with the number of examples used.</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a865b3eee64e9806</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. It involves converting the original floating-point representations of vector components into a lower bit representation, such as from 32-bit floating numbers to 8-bit unsigned integers.\\n\\nProduct quantization, on the other hand, is a different approach to compressing high-dimensional vectors. It involves dividing the vector into smaller sub-vectors and quantizing each sub-vector separately. This method is not as friendly to SIMD (Single Instruction, Multiple Data) operations, which can make distance calculations slower compared to scalar quantization. Additionally, product quantization typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0d4fc07d2f4b8680</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a configuration option that determines whether quantized vectors should be kept cached in RAM at all times. By default, quantized vectors are loaded in the same way as the original vectors. However, if you want to speed up the search process by avoiding the need to load quantized vectors from disk, you can set `always_ram` to `true`. This will ensure that the quantized vectors are stored in RAM, potentially improving search performance at the cost of increased memory usage.</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c730cea5ff8e9206</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The \"vacuum optimizer\" appears to be a configuration setting within a larger system, possibly related to managing and optimizing data storage or indexing. The specific parameters mentioned, such as \"deleted_threshold,\" \"vacuum_min_vector_number,\" \"default_segment_number,\" \"max_segment_size,\" \"memmap_threshold,\" \"indexing_threshold,\" \"flush_interval_sec,\" and \"max_optimization_threads,\" suggest that it deals with the maintenance and optimization of data segments, possibly by removing deleted or obsolete data entries when certain conditions are met, such as the number of vectors falling below a threshold. It may also be responsible for optimizing the performance of the system by managing how data is indexed and stored in memory. The \"flush_interval_sec\" parameter suggests that it performs some operations at regular intervals.</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "212d2c6b46f2474b           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "5b680cfae0d4b4f1           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "0aa7f26ef2ea7615                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "e3d1118a55429921                What is the purpose of ef_construct in HNSW ?   \n",
       "acad6907c9d13ff9            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "048a6afc49aa9229                                 How does oversampling helps?   \n",
       "3861ecba1ca97b94                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "a865b3eee64e9806  What is difference between scalar and product quantization?   \n",
       "0d4fc07d2f4b8680                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "c730cea5ff8e9206                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "212d2c6b46f2474b                                                                                                                                                                                                                                                                                                        The impact of the `write_consistency_factor` is that it determines the number of replicas in a distributed system that must acknowledge a write operation before the system responds to the client. If the `write_consistency_factor` is increased, write operations will be more tolerant to network partitions, as more replicas are required to acknowledge the write. However, this also means that a higher number of active replicas are needed to successfully perform write operations. If not enough replicas are available to meet the `write_consistency_factor`, write operations will not be completed, potentially affecting the availability of the system for writing data.   \n",
       "5b680cfae0d4b4f1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required.   \n",
       "0aa7f26ef2ea7615  The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when sending an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their IDs, payloads, and vectors.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until it has received a response from the leader replica. This ensures that all replicas process the operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.   \n",
       "e3d1118a55429921                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. A larger value for `ef_construct` leads to higher precision in the search but also results in longer indexing time.   \n",
       "acad6907c9d13ff9                                                                                                                                                                                                                                                                                                            The purpose of the `CreatePayloadIndexAsync` method is to create an index for a specific field within a collection in a database. This index is designed to optimize the search and retrieval of data based on the values of that field. The parameters within the method specify the name of the field to be indexed, the schema of the field including its type (e.g., text), the tokenizer to be used (e.g., word), the minimum and maximum token length, and whether the text should be converted to lowercase. This asynchronous operation facilitates efficient querying of text fields by structuring the index to quickly locate entries based on the indexed field's criteria.   \n",
       "048a6afc49aa9229                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Oversampling helps in two distinct ways. In the context of quantization for similarity search algorithms, it allows for significant compression of high-dimensional vectors in memory while compensating for accuracy loss by re-scoring additional points with the original vectors. In the context of training datasets, oversampling helps to equalize the representation of classes, enabling more fair and accurate modeling of real-world scenarios.   \n",
       "3861ecba1ca97b94                                                                                                                                                                                                                                                                                                                                                                                                         The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example. It selects the best positive and best negative scores from these comparisons. The final score for a candidate is determined using a step formula: if the best positive score is greater than the best negative score, the final score is the best positive score; otherwise, the final score is the negative of the best negative score squared. This strategy was introduced in version 1.6.0 and its performance scales linearly with the number of examples used.   \n",
       "a865b3eee64e9806                                                                                                 Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. It involves converting the original floating-point representations of vector components into a lower bit representation, such as from 32-bit floating numbers to 8-bit unsigned integers.\\n\\nProduct quantization, on the other hand, is a different approach to compressing high-dimensional vectors. It involves dividing the vector into smaller sub-vectors and quantizing each sub-vector separately. This method is not as friendly to SIMD (Single Instruction, Multiple Data) operations, which can make distance calculations slower compared to scalar quantization. Additionally, product quantization typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.   \n",
       "0d4fc07d2f4b8680                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The `always_ram` parameter is a configuration option that determines whether quantized vectors should be kept cached in RAM at all times. By default, quantized vectors are loaded in the same way as the original vectors. However, if you want to speed up the search process by avoiding the need to load quantized vectors from disk, you can set `always_ram` to `true`. This will ensure that the quantized vectors are stored in RAM, potentially improving search performance at the cost of increased memory usage.   \n",
       "c730cea5ff8e9206                                                                                                                                The \"vacuum optimizer\" appears to be a configuration setting within a larger system, possibly related to managing and optimizing data storage or indexing. The specific parameters mentioned, such as \"deleted_threshold,\" \"vacuum_min_vector_number,\" \"default_segment_number,\" \"max_segment_size,\" \"memmap_threshold,\" \"indexing_threshold,\" \"flush_interval_sec,\" and \"max_optimization_threads,\" suggest that it deals with the maintenance and optimization of data segments, possibly by removing deleted or obsolete data entries when certain conditions are met, such as the number of vectors falling below a threshold. It may also be responsible for optimizing the performance of the system by managing how data is indexed and stored in memory. The \"flush_interval_sec\" parameter suggests that it performs some operations at regular intervals.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "212d2c6b46f2474b                                                                                                                                                      - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python  \n",
       "5b680cfae0d4b4f1                                                                                                                * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.  \n",
       "0aa7f26ef2ea7615                                                  - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],  \n",
       "e3d1118a55429921                                                                             (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and  \n",
       "acad6907c9d13ff9    client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```  \n",
       "048a6afc49aa9229                                                                                                                                                                                                                                                                                                                                                                                                                                                    ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.  \n",
       "3861ecba1ca97b94  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</as...  \n",
       "a865b3eee64e9806                                       But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.  \n",
       "0d4fc07d2f4b8680  \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...  \n",
       "c730cea5ff8e9206                                                                                       return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,  "
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0ebc273b9cb63812</th>\n",
       "      <th>0</th>\n",
       "      <td>f84efadffd297edc786accdfedbbf1ee</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.832443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f84efadffd297edc786accdfedbbf1ee</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.825855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5762473bc1710405</th>\n",
       "      <th>0</th>\n",
       "      <td>9c6987aff35894e4e124ab2060bcfe40</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>0.821666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9c6987aff35894e4e124ab2060bcfe40</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.787703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6a389c9f00bc1faa</th>\n",
       "      <th>0</th>\n",
       "      <td>45d3a1028595a7d95fcfce1b16ec4251</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.770652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45d3a1028595a7d95fcfce1b16ec4251</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "      <td>0.740061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">331a47badf78052a</th>\n",
       "      <th>0</th>\n",
       "      <td>3bf9cc3bc3fb4169328e7c7ab4470445</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.787150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3bf9cc3bc3fb4169328e7c7ab4470445</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "      <td>0.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2355564201a39480</th>\n",
       "      <th>0</th>\n",
       "      <td>e5c8605ba444188fe9d1639dfecf49de</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.717572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e5c8605ba444188fe9d1639dfecf49de</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "      <td>0.698218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">badcc3e615a03133</th>\n",
       "      <th>0</th>\n",
       "      <td>eb671173e46f6b8543ad206aa44a7d19</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.819349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb671173e46f6b8543ad206aa44a7d19</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>0.815198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20b6b1f2d1ed93cf</th>\n",
       "      <th>0</th>\n",
       "      <td>19ef6a8ef2ee19d4e5c93b5a13f6faa2</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.825645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19ef6a8ef2ee19d4e5c93b5a13f6faa2</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/aside&gt;</td>\n",
       "      <td>0.808645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">c3c401a7cd8a15d5</th>\n",
       "      <th>0</th>\n",
       "      <td>db202bc6aca4c958782433671dcb70df</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.857293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>db202bc6aca4c958782433671dcb70df</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.848822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">c873ebc109dda9ee</th>\n",
       "      <th>0</th>\n",
       "      <td>186d0b85a51b7a6bb7814417c3e64764</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.777294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186d0b85a51b7a6bb7814417c3e64764</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ab8df71a4dadf6c9</th>\n",
       "      <th>0</th>\n",
       "      <td>030016fe6a5c20ddfef6c29424184bba</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.718610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>030016fe6a5c20ddfef6c29424184bba</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "      <td>0.707060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "0ebc273b9cb63812 0                  f84efadffd297edc786accdfedbbf1ee   \n",
       "                 1                  f84efadffd297edc786accdfedbbf1ee   \n",
       "5762473bc1710405 0                  9c6987aff35894e4e124ab2060bcfe40   \n",
       "                 1                  9c6987aff35894e4e124ab2060bcfe40   \n",
       "6a389c9f00bc1faa 0                  45d3a1028595a7d95fcfce1b16ec4251   \n",
       "                 1                  45d3a1028595a7d95fcfce1b16ec4251   \n",
       "331a47badf78052a 0                  3bf9cc3bc3fb4169328e7c7ab4470445   \n",
       "                 1                  3bf9cc3bc3fb4169328e7c7ab4470445   \n",
       "2355564201a39480 0                  e5c8605ba444188fe9d1639dfecf49de   \n",
       "                 1                  e5c8605ba444188fe9d1639dfecf49de   \n",
       "badcc3e615a03133 0                  eb671173e46f6b8543ad206aa44a7d19   \n",
       "                 1                  eb671173e46f6b8543ad206aa44a7d19   \n",
       "20b6b1f2d1ed93cf 0                  19ef6a8ef2ee19d4e5c93b5a13f6faa2   \n",
       "                 1                  19ef6a8ef2ee19d4e5c93b5a13f6faa2   \n",
       "c3c401a7cd8a15d5 0                  db202bc6aca4c958782433671dcb70df   \n",
       "                 1                  db202bc6aca4c958782433671dcb70df   \n",
       "c873ebc109dda9ee 0                  186d0b85a51b7a6bb7814417c3e64764   \n",
       "                 1                  186d0b85a51b7a6bb7814417c3e64764   \n",
       "ab8df71a4dadf6c9 0                  030016fe6a5c20ddfef6c29424184bba   \n",
       "                 1                  030016fe6a5c20ddfef6c29424184bba   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "0ebc273b9cb63812 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "5762473bc1710405 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "6a389c9f00bc1faa 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "331a47badf78052a 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "2355564201a39480 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "badcc3e615a03133 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "20b6b1f2d1ed93cf 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "c3c401a7cd8a15d5 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "c873ebc109dda9ee 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "ab8df71a4dadf6c9 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "0ebc273b9cb63812 0                                                                                                                                                                                                                                                       - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "                 1                                  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "5762473bc1710405 0                                                                                                  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                                 The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "6a389c9f00bc1faa 0                                                                                                                                                                   - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "                 1                  ```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],   \n",
       "331a47badf78052a 0                                                                  (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "                 1                                                                                                                                              The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and   \n",
       "2355564201a39480 0                                                                                   client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "                 1                                                    },\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```   \n",
       "badcc3e615a03133 0                                                                                                                                                            ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 1                                                                                                                                                                                                                                                                                                                                                                                                                           oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "20b6b1f2d1ed93cf 0                                                                                  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "                 1                                                  The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</aside>   \n",
       "c3c401a7cd8a15d5 0                                                                                           But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "                 1                                                                               *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "c873ebc109dda9ee 0                                                                       \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "                 1                                                          It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "ab8df71a4dadf6c9 0                                                                                                                                                                 return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "                 1                                                         },\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "0ebc273b9cb63812 0                        0.832443  \n",
       "                 1                        0.825855  \n",
       "5762473bc1710405 0                        0.821666  \n",
       "                 1                        0.787703  \n",
       "6a389c9f00bc1faa 0                        0.770652  \n",
       "                 1                        0.740061  \n",
       "331a47badf78052a 0                        0.787150  \n",
       "                 1                        0.767757  \n",
       "2355564201a39480 0                        0.717572  \n",
       "                 1                        0.698218  \n",
       "badcc3e615a03133 0                        0.819349  \n",
       "                 1                        0.815198  \n",
       "20b6b1f2d1ed93cf 0                        0.825645  \n",
       "                 1                        0.808645  \n",
       "c3c401a7cd8a15d5 0                        0.857293  \n",
       "                 1                        0.848822  \n",
       "c873ebc109dda9ee 0                        0.777294  \n",
       "                 1                        0.765625  \n",
       "ab8df71a4dadf6c9 0                        0.718610  \n",
       "                 1                        0.707060  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. Define your evaluation model and your evaluators for Hybrid Search**\n",
    "\n",
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10fffb2d8e24e0a9057bc0937e9b748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf9994f4d9b44cbb01a328f52ed7b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df_hybrid, qa_correctness_eval_df_hybrid = run_evals(\n",
    "    dataframe=queries_df_hybrid,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df_hybrid = run_evals(\n",
    "    dataframe=retrieved_documents_df_hybrid,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df_hybrid),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df_hybrid),\n",
    "    project_name=HYBRID_RAG_PROJECT,\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df_hybrid),\n",
    "                            project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003c1412566441b2a0fd878aabed07a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01d4ff90aac943ff83bfe49b70c783ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d9844dfef3f414aa750e6200a62bfd3",
       "IPY_MODEL_57d8cc8ff7f047c19301f0793367b2ae",
       "IPY_MODEL_4b23a2eb73b6435384686ced47f9c7f6"
      ],
      "layout": "IPY_MODEL_6fc5bf4070d94eccbdcb4fad4247c758"
     }
    },
    "02ca3a5d43e343d5b756fca172bc1822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1e6788ea4344553adbe66202caa656c",
       "IPY_MODEL_5a3d5d83788446028f4de4afbd262f3e",
       "IPY_MODEL_bf06bcbdbff3485abb3895dc54256e00"
      ],
      "layout": "IPY_MODEL_1ff9ec98314944bdb86d4fda42a4c88b"
     }
    },
    "0685ef29e4c54e2e8d5c424508ebfe82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "080a6d0c05144698a9c7366e54bb39b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f86e93b0fe48ddbaf2cabc9975c68b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8e261a6071cd41a6afd4377c9ead98ca",
      "value": "‚Äá35/35‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:37&lt;00:00‚Äá|‚Äá‚Äá1.72it/s"
     }
    },
    "0a8c4c23041641308b625e929790a55e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c90d55a6f6d4d3f8c72bd0ba19687ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be68eca0c7b640abada072a3cc3cd042",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6eadc3109208449eb9cb7ad5b6a5bb46",
      "value": "‚Äá125k/125k‚Äá[00:01&lt;00:00,‚Äá84.2kB/s]"
     }
    },
    "0d229e81fcb246cf99dd919d06cf5905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1057e11184c44ea38154a2ceabd90198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4722676ae36d4749918f5edaac48b182",
       "IPY_MODEL_a2041da6fb5f499191f405fa891c0907",
       "IPY_MODEL_5c26103d1d89466c9d22960bb347db39"
      ],
      "layout": "IPY_MODEL_0685ef29e4c54e2e8d5c424508ebfe82"
     }
    },
    "10d5d30b4fd14f3ba45da1794607c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "128b89e88c684401818304e8410b6c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1677f41c03a74a9f8ac55c49a9c12799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bda9ff3e5071497c933f31a2dd44b449",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b6686121f3084e1a8c6e0399a8a72675",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "178bbf06a9ae4b52baa5b731c0c81590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003c1412566441b2a0fd878aabed07a6",
      "max": 4431,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf1c5baa009c467a8c76a1a78d6987b1",
      "value": 4431
     }
    },
    "18ee1349c9394dab81163d5de23a04a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ad21fe66fac4742beb7c17ba8f8733f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1677f41c03a74a9f8ac55c49a9c12799",
       "IPY_MODEL_5b8040a4238f41128006185a858db466",
       "IPY_MODEL_7fbfaf28e7134c638b3e4d16e3cb3e6c"
      ],
      "layout": "IPY_MODEL_1f1e8f2a35b645eb91e96a847cf2856b"
     }
    },
    "1e846da2f0c44e099d24c350e0b0eb1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f1e8f2a35b645eb91e96a847cf2856b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3117c63e8d4558b801a33e9b5c548a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff9ec98314944bdb86d4fda42a4c88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218ea3901010405a95033a6cb632250a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e41bb05ed34b02b2e9e1c2d7d56145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c94d8ddc9d4561930044518630bb0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ee1349c9394dab81163d5de23a04a8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_21e41bb05ed34b02b2e9e1c2d7d56145",
      "value": "‚Äá4431/4431‚Äá[00:02&lt;00:00,‚Äá2155.10it/s]"
     }
    },
    "251b1e790043469393aae0c8ebec3d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280b4a0048134a8aaf523deead4e3aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "366e10ebb46a45508d837c8ead93124c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a222b53b0b2448a97b23e2b782a37b5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99f0a6ea567247639e7b17af1e1df9cf",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "37a8add4c6a242a49ea09ebef29cfdd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993aeba0d0f4021b4ae31b8378af903",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfbcd04f19374fe6b8ab9802f6891612",
      "value": "‚Äá240/0‚Äá[00:00&lt;00:00,‚Äá2042.22‚Äáexamples/s]"
     }
    },
    "38d701e482684695b97bd73bac78ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43dfd4070e324858974c08fbcb8055fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43e63c6009c14d9cb4eee3ea177d1c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f997d5f3659421981252a05845dd236",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_af7527216b9f4240b36024d07ac5c507",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá946B/s]"
     }
    },
    "44cb1b5c1c0a446f9d308d93d33ae4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50d006eafd294bbab3339c2bece90673",
      "max": 335,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21c626648d643d4972c76a094391134",
      "value": 335
     }
    },
    "4722676ae36d4749918f5edaac48b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3888c2a5954fb29293f41577a5bcae",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f7b5096a4b944d62a281ee927a1ce2e4",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "4ada07ba27b14c1a8c071343cc21c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b23a2eb73b6435384686ced47f9c7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9fa5a2efb1142c9abd47fe46f05cee6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_56dc39917eec4eba8194bbe0a3e91de1",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá1.71kB/s]"
     }
    },
    "4cc3f02b7bb44a87b8d50dd2f98edce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "503e9200b403425b8c0379b0e74b01fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50d006eafd294bbab3339c2bece90673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54aa6aff06214fe4baa3e621ec306c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "56dc39917eec4eba8194bbe0a3e91de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d8cc8ff7f047c19301f0793367b2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ce212a2baa494a8a9ae439c7c3e742",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f61b254bd9e24480b0176b43c1c7f47e",
      "value": 43
     }
    },
    "5a3d5d83788446028f4de4afbd262f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc3f02b7bb44a87b8d50dd2f98edce7",
      "max": 1777260,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e07726709c7748eeb708dbe5c37f9770",
      "value": 1777260
     }
    },
    "5b8040a4238f41128006185a858db466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_503e9200b403425b8c0379b0e74b01fe",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f6e8b35568945dc85ba3e47f7b0f3d7",
      "value": 2048
     }
    },
    "5c26103d1d89466c9d22960bb347db39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ada07ba27b14c1a8c071343cc21c9c4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_98d7cd89900f44a79893e7d8d6b8b0c7",
      "value": "‚Äá81/0‚Äá[00:00&lt;00:00,‚Äá1674.70‚Äáexamples/s]"
     }
    },
    "5c65981c04f34589a36c5c8bb4195f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cbb12c59ec948ba8d66dc3812d5c846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602df92e53334c65a2cc171301c67e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65f86e93b0fe48ddbaf2cabc9975c68b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66588b1f23bb421db7dbc90b2298a845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68ac22f645934827a9627e3559067ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3c5e97c2ac94cafa20b130f1f45ce67",
       "IPY_MODEL_b505d5e7978d4b19b59dc8f3fe71e795",
       "IPY_MODEL_080a6d0c05144698a9c7366e54bb39b0"
      ],
      "layout": "IPY_MODEL_e69da775e4df4ed8a5f5ed1348278ab8"
     }
    },
    "6a222b53b0b2448a97b23e2b782a37b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b115bdeaeb547e6b96a5a440b2fe3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eadc3109208449eb9cb7ad5b6a5bb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f6e8b35568945dc85ba3e47f7b0f3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fc5bf4070d94eccbdcb4fad4247c758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ff26418624f4662871a69796f67427a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72ce212a2baa494a8a9ae439c7c3e742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d79ac84c8c4d8393fcdb7f304b4bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79335cc0bf85458ba630ced71d706b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_251b1e790043469393aae0c8ebec3d77",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_602df92e53334c65a2cc171301c67e46",
      "value": 43
     }
    },
    "7b8b3e1c571a4345bc2ec4eb00f0967e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a74d6dce715f4494874de92852b56c09",
       "IPY_MODEL_a5d80d3579ca481a98310bd08b8c6918",
       "IPY_MODEL_f07a59b5bc7a4f56baa892ca864d488c"
      ],
      "layout": "IPY_MODEL_84fd76a737334831a886755affb50886"
     }
    },
    "7e8c241b51af4963aa78d91e6e78136e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fbfaf28e7134c638b3e4d16e3cb3e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c47e7205eb6540a8a5c51120cd222fe3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ea74a3a2d06a4c61b85db3c7552535a1",
      "value": "‚Äá2048/2048‚Äá[00:20&lt;00:00,‚Äá120.27it/s]"
     }
    },
    "8115b0b37d164080b58368be3fadbaa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84fd76a737334831a886755affb50886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869c8dd95cf7452b841f7203e0e6b8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9844dfef3f414aa750e6200a62bfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218ea3901010405a95033a6cb632250a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8ef997a7dbf54d3d9cfa8147e1611cbd",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "8e261a6071cd41a6afd4377c9ead98ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ef997a7dbf54d3d9cfa8147e1611cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f997d5f3659421981252a05845dd236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90f264886e864ddabdb6eddbecf6d75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "924c8fe0a9cb4feab39d31ec358089b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98d7cd89900f44a79893e7d8d6b8b0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "997305a8d871401ba6a1e805dc1ca045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99f0a6ea567247639e7b17af1e1df9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f88771edbfd468cb315a54b69eb7226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c3867b1d404a948a0edd0e6bc9a1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff9f4b2c32234207acb5221babe9c061",
      "max": 124978,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51b6ff6e830489f8e1e100bf1f1ae98",
      "value": 124978
     }
    },
    "a2041da6fb5f499191f405fa891c0907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54aa6aff06214fe4baa3e621ec306c04",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66588b1f23bb421db7dbc90b2298a845",
      "value": 1
     }
    },
    "a5d80d3579ca481a98310bd08b8c6918": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce0efd7352cd4539bda8706ebbc146fd",
      "max": 22,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128b89e88c684401818304e8410b6c33",
      "value": 22
     }
    },
    "a74d6dce715f4494874de92852b56c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c65981c04f34589a36c5c8bb4195f2b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_10d5d30b4fd14f3ba45da1794607c5e0",
      "value": "run_evals‚Äá"
     }
    },
    "aa8db2c8310b4f9582f2002adf3ffa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b84c10312dbe486fb3b21c7c5e23d603",
       "IPY_MODEL_178bbf06a9ae4b52baa5b731c0c81590",
       "IPY_MODEL_22c94d8ddc9d4561930044518630bb0b"
      ],
      "layout": "IPY_MODEL_acf3351a0fba41558f97945fbc6b84c7"
     }
    },
    "ac507e1d00b446df9379fa58f39418fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85db136dc2843a58e7aa648bbf9f9fe",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5cbb12c59ec948ba8d66dc3812d5c846",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "acf3351a0fba41558f97945fbc6b84c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adc2166e9b7d4ebdb5dd5415fd3e2ec5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "af7527216b9f4240b36024d07ac5c507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af77bbc36b844afa960381389111bccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7d2403f4504d49a027d935b13d3f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ff26418624f4662871a69796f67427a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5e93af9e5644c419bc653af2057e285",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "b21c626648d643d4972c76a094391134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3dc75c3ec7a48ceaff045c81f250bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b44f529954e944729b7446a4d610ac1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b505d5e7978d4b19b59dc8f3fe71e795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38d701e482684695b97bd73bac78ccfc",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7d21aa7bce145b78e834897b158f9b0",
      "value": 35
     }
    },
    "b51b292bc0b341de8fd94707e7fa2c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8749c8627c44757aa8703b45421af2d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9f88771edbfd468cb315a54b69eb7226",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "b6686121f3084e1a8c6e0399a8a72675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b84c10312dbe486fb3b21c7c5e23d603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f3117c63e8d4558b801a33e9b5c548a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_997305a8d871401ba6a1e805dc1ca045",
      "value": "Parsing‚Äánodes:‚Äá100%"
     }
    },
    "b993aeba0d0f4021b4ae31b8378af903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3888c2a5954fb29293f41577a5bcae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda9ff3e5071497c933f31a2dd44b449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be68eca0c7b640abada072a3cc3cd042": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf06bcbdbff3485abb3895dc54256e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8115b0b37d164080b58368be3fadbaa0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d229e81fcb246cf99dd919d06cf5905",
      "value": "‚Äá1.78M/1.78M‚Äá[00:00&lt;00:00,‚Äá4.56MB/s]"
     }
    },
    "c30b902e82de486eb0c10a3e64687b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a80fdcb9944e62ba5b51838fef5ef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af7d2403f4504d49a027d935b13d3f62",
       "IPY_MODEL_79335cc0bf85458ba630ced71d706b2e",
       "IPY_MODEL_43e63c6009c14d9cb4eee3ea177d1c7d"
      ],
      "layout": "IPY_MODEL_dce488c3561e4d7fa737379553ec0b3b"
     }
    },
    "c47e7205eb6540a8a5c51120cd222fe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c762c26d4f2e4159b714ad55bbd195eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b51b292bc0b341de8fd94707e7fa2c0c",
       "IPY_MODEL_f10f2f679d7a4ab8988601cd59aa4b70",
       "IPY_MODEL_d5baee84346c4dc29ffa4fd2fe6495c6"
      ],
      "layout": "IPY_MODEL_0a8c4c23041641308b625e929790a55e"
     }
    },
    "c85db136dc2843a58e7aa648bbf9f9fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95d4cfffece443182faa3e2a3e48dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df997aad597e4c1ab23501d12383e39b",
       "IPY_MODEL_a1c3867b1d404a948a0edd0e6bc9a1b8",
       "IPY_MODEL_0c90d55a6f6d4d3f8c72bd0ba19687ba"
      ],
      "layout": "IPY_MODEL_1e846da2f0c44e099d24c350e0b0eb1f"
     }
    },
    "ce0efd7352cd4539bda8706ebbc146fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1c5baa009c467a8c76a1a78d6987b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfbcd04f19374fe6b8ab9802f6891612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cff10cfd18704a9cb7c9b4d4ba5ccaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d44a32f577204732a7f4ce767d8e83f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d51b6ff6e830489f8e1e100bf1f1ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5baee84346c4dc29ffa4fd2fe6495c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_280b4a0048134a8aaf523deead4e3aef",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_869c8dd95cf7452b841f7203e0e6b8c5",
      "value": "‚Äá2048/2048‚Äá[00:19&lt;00:00,‚Äá119.05it/s]"
     }
    },
    "d5e93af9e5644c419bc653af2057e285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d772ecb46ca54adbbb84749fe07ab735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8749c8627c44757aa8703b45421af2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9fa5a2efb1142c9abd47fe46f05cee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce488c3561e4d7fa737379553ec0b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df997aad597e4c1ab23501d12383e39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_924c8fe0a9cb4feab39d31ec358089b8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "e07726709c7748eeb708dbe5c37f9770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e69da775e4df4ed8a5f5ed1348278ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7d21aa7bce145b78e834897b158f9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7d4e009f37a4d798dbc23b39ed7c0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea74a3a2d06a4c61b85db3c7552535a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb848aa889474b118c49a7cdac509d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f264886e864ddabdb6eddbecf6d75a",
      "value": 1
     }
    },
    "eecec2323e294d6fb4ee89f5937c241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac507e1d00b446df9379fa58f39418fc",
       "IPY_MODEL_eb848aa889474b118c49a7cdac509d9b",
       "IPY_MODEL_37a8add4c6a242a49ea09ebef29cfdd9"
      ],
      "layout": "IPY_MODEL_b44f529954e944729b7446a4d610ac1d"
     }
    },
    "f07a59b5bc7a4f56baa892ca864d488c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d4e009f37a4d798dbc23b39ed7c0b1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d772ecb46ca54adbbb84749fe07ab735",
      "value": "‚Äá22/22‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:52&lt;00:00‚Äá|‚Äá‚Äá2.31it/s"
     }
    },
    "f10f2f679d7a4ab8988601cd59aa4b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af77bbc36b844afa960381389111bccf",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3dc75c3ec7a48ceaff045c81f250bfd",
      "value": 2048
     }
    },
    "f1e6788ea4344553adbe66202caa656c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e47bbf438740a2a224c5f7d913e502",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c30b902e82de486eb0c10a3e64687b44",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "f3c5e97c2ac94cafa20b130f1f45ce67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77d79ac84c8c4d8393fcdb7f304b4bf3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_43dfd4070e324858974c08fbcb8055fd",
      "value": "run_evals‚Äá"
     }
    },
    "f61b254bd9e24480b0176b43c1c7f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7b5096a4b944d62a281ee927a1ce2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e47bbf438740a2a224c5f7d913e502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa26918ac71440c1803c82c939fbc79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_366e10ebb46a45508d837c8ead93124c",
       "IPY_MODEL_44cb1b5c1c0a446f9d308d93d33ae4f8",
       "IPY_MODEL_fca056b08fc3427a977bc3717a020443"
      ],
      "layout": "IPY_MODEL_7e8c241b51af4963aa78d91e6e78136e"
     }
    },
    "fca056b08fc3427a977bc3717a020443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44a32f577204732a7f4ce767d8e83f5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6b115bdeaeb547e6b96a5a440b2fe3ef",
      "value": "‚Äá335/335‚Äá[00:05&lt;00:00,‚Äá59.78it/s]"
     }
    },
    "ff9f4b2c32234207acb5221babe9c061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
