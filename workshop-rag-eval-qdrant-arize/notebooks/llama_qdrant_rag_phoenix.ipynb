{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmb4fjQvQmFT"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/qdrant_arize.png\" width=\"500\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tuning a RAG Pipeline using Qdrant and Arize Phoenix</h1>\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Relevant Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jCyVyMs-JrWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "HYBRID_RAG_PROJECT = \"hybrid-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NNJI9dP6GeUE"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import ssl\n",
    "import time\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import certifi\n",
    "import nest_asyncio\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, download_loader,\n",
    "    load_index_from_storage, set_global_handler\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator, OpenAIModel, QAEvaluator,\n",
    "    RelevanceEvaluator, run_evals\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnr-p5J6jyEl"
   },
   "source": [
    "### **2. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "zYfgwhXvZcOV",
    "outputId": "3599030d-8ba2-4f8a-c8ec-a0d5b576015b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9n105Mckm1f"
   },
   "source": [
    "Be sure to enable phoenix as your global handler for tracing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1Ym-djTOkjxQ"
   },
   "outputs": [],
   "source": [
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z15BcuTcj6Cp"
   },
   "source": [
    "### **3. Setup your openai key and retrieve the documents to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EsFBezzcGbG",
    "outputId": "158775a0-8f34-4201-88c5-b221fe40f883"
   },
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieve the documents / dataset to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c3a80fdcb9944e62ba5b51838fef5ef4",
      "af7d2403f4504d49a027d935b13d3f62",
      "79335cc0bf85458ba630ced71d706b2e",
      "43e63c6009c14d9cb4eee3ea177d1c7d",
      "dce488c3561e4d7fa737379553ec0b3b",
      "6ff26418624f4662871a69796f67427a",
      "d5e93af9e5644c419bc653af2057e285",
      "251b1e790043469393aae0c8ebec3d77",
      "602df92e53334c65a2cc171301c67e46",
      "8f997d5f3659421981252a05845dd236",
      "af7527216b9f4240b36024d07ac5c507",
      "02ca3a5d43e343d5b756fca172bc1822",
      "f1e6788ea4344553adbe66202caa656c",
      "5a3d5d83788446028f4de4afbd262f3e",
      "bf06bcbdbff3485abb3895dc54256e00",
      "1ff9ec98314944bdb86d4fda42a4c88b",
      "f9e47bbf438740a2a224c5f7d913e502",
      "c30b902e82de486eb0c10a3e64687b44",
      "4cc3f02b7bb44a87b8d50dd2f98edce7",
      "e07726709c7748eeb708dbe5c37f9770",
      "8115b0b37d164080b58368be3fadbaa0",
      "0d229e81fcb246cf99dd919d06cf5905",
      "eecec2323e294d6fb4ee89f5937c241c",
      "ac507e1d00b446df9379fa58f39418fc",
      "eb848aa889474b118c49a7cdac509d9b",
      "37a8add4c6a242a49ea09ebef29cfdd9",
      "b44f529954e944729b7446a4d610ac1d",
      "c85db136dc2843a58e7aa648bbf9f9fe",
      "5cbb12c59ec948ba8d66dc3812d5c846",
      "adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "90f264886e864ddabdb6eddbecf6d75a",
      "b993aeba0d0f4021b4ae31b8378af903",
      "cfbcd04f19374fe6b8ab9802f6891612"
     ]
    },
    "id": "iisWzELAzYca",
    "outputId": "751b3676-2a8f-4497-de7e-85ee3e9ec199"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"atitaarora/qdrant_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5jYTnnlza6b",
    "outputId": "2894f142-0d41-42b8-b75d-63850ad8a2db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='csv', dataset_name='qdrant_doc', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=1767967, num_examples=240, shard_lengths=None, dataset_name='qdrant_doc')}, download_checksums={'hf://datasets/atitaarora/qdrant_doc@8d859890840f65337c38e96d660b81b1441bbecd/documents.csv': {'num_bytes': 1777260, 'checksum': None}}, download_size=1777260, post_processing_size=None, dataset_size=1767967, size_in_bytes=3545227)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Definition of global chunk properties and chunk processing**\n",
    "Processing each document with desired **TEXT_SPLITTER_ALGO , CHUNK_SIZE , CHUNK_OVERLAP** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global config for chunk processing\n",
    "CHUNK_SIZE = 512 #1000\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Process dataset as langchain (or llamaindex) document for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWTlffFW6Art",
    "outputId": "2685adf0-2965-4c2d-878f-67eee384d65c"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.core import Document\n",
    "\n",
    "## Split and process the document chunks from the given dataset\n",
    "\n",
    "def process_document_chunks(dataset,chunk_size,chunk_overlap):\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(dataset)\n",
    "    ]\n",
    "\n",
    "    # could showcase another variation of processed documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    ## Converting Langchain document chunks above into Llamaindex Document for ingestion\n",
    "    llama_documents = [\n",
    "        Document.from_langchain_format(doc)\n",
    "        for doc in docs_processed\n",
    "    ]\n",
    "    return llama_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbWBhR_661SX",
    "outputId": "79b17993-361d-47e1-ec14-f6f6dfd6949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 14011.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = process_document_chunks(dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5DrAtMkbL-"
   },
   "source": [
    "### **7. Setting up Qdrant and Collection**\n",
    "\n",
    "We first set up the qdrant client and then create a collection so that our data may be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QYir5ueWcb8r"
   },
   "outputs": [],
   "source": [
    "##Uncomment to initialise qdrant client in memory\n",
    "#client = qdrant_client.QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "#)\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collection Name \n",
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-IUMQAT76Ep",
    "outputId": "88207dc6-c291-4d76-c8e9-ea6f180ed02a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Collection level operations\n",
    "\n",
    "## Get information about existing collections \n",
    "client.get_collections()\n",
    "\n",
    "## Get information about specific collection\n",
    "#collection_info = client.get_collection(COLLECTION_NAME)\n",
    "#print(collection_info)\n",
    "\n",
    "## Deleting collection, if need be\n",
    "#client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>description</th>\n",
       "      <th>size_in_GB</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/bge-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model</td>\n",
       "      <td>0.420</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model, v1.5</td>\n",
       "      <td>0.210</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large English model, v1.5</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/bge-large-en-v1.5-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/bge-small-en</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast English model</td>\n",
       "      <td>0.130</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast and Default English model</td>\n",
       "      <td>0.067</td>\n",
       "      <td>{'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAAI/bge-small-zh-v1.5</td>\n",
       "      <td>512</td>\n",
       "      <td>Fast and recommended Chinese model</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, MiniLM-L6-v2</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>0.220</td>\n",
       "      <td>{'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1.5'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thenlper/gte-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large general text embeddings model</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/gte-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mixedbread-ai/mxbai-embed-large-v1</td>\n",
       "      <td>1024</td>\n",
       "      <td>MixedBread Base sentence embedding model, does well on MTEB</td>\n",
       "      <td>0.640</td>\n",
       "      <td>{'hf': 'mixedbread-ai/mxbai-embed-large-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>intfloat/multilingual-e5-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Multilingual model, e5-large. Recommend using this model for non-English languages</td>\n",
       "      <td>2.240</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>768</td>\n",
       "      <td>Sentence-transformers model for tasks like clustering or semantic search</td>\n",
       "      <td>1.000</td>\n",
       "      <td>{'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-base-en'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jinaai/jina-embeddings-v2-small-en</td>\n",
       "      <td>512</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.120</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-small-en'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          model   dim  \\\n",
       "0                                              BAAI/bge-base-en   768   \n",
       "1                                         BAAI/bge-base-en-v1.5   768   \n",
       "2                                        BAAI/bge-large-en-v1.5  1024   \n",
       "3                                             BAAI/bge-small-en   384   \n",
       "4                                        BAAI/bge-small-en-v1.5   384   \n",
       "5                                        BAAI/bge-small-zh-v1.5   512   \n",
       "6                        sentence-transformers/all-MiniLM-L6-v2   384   \n",
       "7   sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2   384   \n",
       "8                                  nomic-ai/nomic-embed-text-v1   768   \n",
       "9                                nomic-ai/nomic-embed-text-v1.5   768   \n",
       "10                                           thenlper/gte-large  1024   \n",
       "11                           mixedbread-ai/mxbai-embed-large-v1  1024   \n",
       "12                               intfloat/multilingual-e5-large  1024   \n",
       "13  sentence-transformers/paraphrase-multilingual-mpnet-base-v2   768   \n",
       "14                            jinaai/jina-embeddings-v2-base-en   768   \n",
       "15                           jinaai/jina-embeddings-v2-small-en   512   \n",
       "\n",
       "                                                                           description  \\\n",
       "0                                                                   Base English model   \n",
       "1                                                             Base English model, v1.5   \n",
       "2                                                            Large English model, v1.5   \n",
       "3                                                                   Fast English model   \n",
       "4                                                       Fast and Default English model   \n",
       "5                                                   Fast and recommended Chinese model   \n",
       "6                                             Sentence Transformer model, MiniLM-L6-v2   \n",
       "7                    Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2   \n",
       "8                                                    8192 context length english model   \n",
       "9                                                    8192 context length english model   \n",
       "10                                                 Large general text embeddings model   \n",
       "11                         MixedBread Base sentence embedding model, does well on MTEB   \n",
       "12  Multilingual model, e5-large. Recommend using this model for non-English languages   \n",
       "13            Sentence-transformers model for tasks like clustering or semantic search   \n",
       "14                             English embedding model supporting 8192 sequence length   \n",
       "15                             English embedding model supporting 8192 sequence length   \n",
       "\n",
       "    size_in_GB  \\\n",
       "0        0.420   \n",
       "1        0.210   \n",
       "2        1.200   \n",
       "3        0.130   \n",
       "4        0.067   \n",
       "5        0.090   \n",
       "6        0.090   \n",
       "7        0.220   \n",
       "8        0.520   \n",
       "9        0.520   \n",
       "10       1.200   \n",
       "11       0.640   \n",
       "12       2.240   \n",
       "13       1.000   \n",
       "14       0.520   \n",
       "15       0.120   \n",
       "\n",
       "                                                                                                                                           sources  \n",
       "0                                                               {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}  \n",
       "1                  {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}  \n",
       "2                                                                                                          {'hf': 'qdrant/bge-large-en-v1.5-onnx'}  \n",
       "3                                                              {'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}  \n",
       "4                                                                                                        {'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}  \n",
       "5                                                         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}  \n",
       "6   {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}  \n",
       "7                                                                                    {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}  \n",
       "8                                                                                                           {'hf': 'nomic-ai/nomic-embed-text-v1'}  \n",
       "9                                                                                                         {'hf': 'nomic-ai/nomic-embed-text-v1.5'}  \n",
       "10                                                                                                                 {'hf': 'qdrant/gte-large-onnx'}  \n",
       "11                                                                                                    {'hf': 'mixedbread-ai/mxbai-embed-large-v1'}  \n",
       "12         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}  \n",
       "13                                                                                          {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}  \n",
       "14                                                                                                     {'hf': 'xenova/jina-embeddings-v2-base-en'}  \n",
       "15                                                                                                    {'hf': 'xenova/jina-embeddings-v2-small-en'}  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Declaring the intended Embedding Model with Fastembed\n",
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "pd.DataFrame(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Document Embedding processing and Ingestion**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and creates a new collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "aa8db2c8310b4f9582f2002adf3ffa1b",
      "b84c10312dbe486fb3b21c7c5e23d603",
      "178bbf06a9ae4b52baa5b731c0c81590",
      "22c94d8ddc9d4561930044518630bb0b",
      "acf3351a0fba41558f97945fbc6b84c7",
      "1f3117c63e8d4558b801a33e9b5c548a",
      "997305a8d871401ba6a1e805dc1ca045",
      "003c1412566441b2a0fd878aabed07a6",
      "cf1c5baa009c467a8c76a1a78d6987b1",
      "18ee1349c9394dab81163d5de23a04a8",
      "21e41bb05ed34b02b2e9e1c2d7d56145",
      "c762c26d4f2e4159b714ad55bbd195eb",
      "b51b292bc0b341de8fd94707e7fa2c0c",
      "f10f2f679d7a4ab8988601cd59aa4b70",
      "d5baee84346c4dc29ffa4fd2fe6495c6",
      "0a8c4c23041641308b625e929790a55e",
      "d8749c8627c44757aa8703b45421af2d",
      "9f88771edbfd468cb315a54b69eb7226",
      "af77bbc36b844afa960381389111bccf",
      "b3dc75c3ec7a48ceaff045c81f250bfd",
      "280b4a0048134a8aaf523deead4e3aef",
      "869c8dd95cf7452b841f7203e0e6b8c5",
      "1ad21fe66fac4742beb7c17ba8f8733f",
      "1677f41c03a74a9f8ac55c49a9c12799",
      "5b8040a4238f41128006185a858db466",
      "7fbfaf28e7134c638b3e4d16e3cb3e6c",
      "1f1e8f2a35b645eb91e96a847cf2856b",
      "bda9ff3e5071497c933f31a2dd44b449",
      "b6686121f3084e1a8c6e0399a8a72675",
      "503e9200b403425b8c0379b0e74b01fe",
      "6f6e8b35568945dc85ba3e47f7b0f3d7",
      "c47e7205eb6540a8a5c51120cd222fe3",
      "ea74a3a2d06a4c61b85db3c7552535a1",
      "fa26918ac71440c1803c82c939fbc79f",
      "366e10ebb46a45508d837c8ead93124c",
      "44cb1b5c1c0a446f9d308d93d33ae4f8",
      "fca056b08fc3427a977bc3717a020443",
      "7e8c241b51af4963aa78d91e6e78136e",
      "6a222b53b0b2448a97b23e2b782a37b5",
      "99f0a6ea567247639e7b17af1e1df9cf",
      "50d006eafd294bbab3339c2bece90673",
      "b21c626648d643d4972c76a094391134",
      "d44a32f577204732a7f4ce767d8e83f5",
      "6b115bdeaeb547e6b96a5a440b2fe3ef"
     ]
    },
    "id": "DTjwoobJcJO3",
    "outputId": "1dcd8c41-4066-4130-f20f-dabc2a1f31d5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ee7aa6cc3643cd8f0baebafa6028a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4638bb04ed9546559b7486e601669872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551ae99eab7b4d7a91d2652f5530ca78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e573239c03d74f5fa4d4762e4690d4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce491a1400b460d816c2b67c042c36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from phoenix.trace import suppress_tracing\n",
    "## Uncomment it if you'd like to use FastEmbed instead of OpenAI\n",
    "## For the complete list of supported models,\n",
    "##please check https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "##Uncomment if using FastEmbed\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Uncomment it if you'd like to use OpenAI Embeddings instead of FastEmbed\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n",
    "\n",
    "with suppress_tracing():\n",
    "  index = VectorStoreIndex.from_documents(\n",
    "      documents,\n",
    "      storage_context=storage_context,\n",
    "      show_progress=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8a. Connecting to existing Collection**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and uses the previously generated collection to work fully connected with Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment it if using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52PEPokb8FlO",
    "outputId": "1cfa3ea9-09e1-4fdd-eed4-2aa8764ea4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nxhA_6plBSL"
   },
   "source": [
    "### **9.Running an example query and printing out the response.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Zlep2wVsf1tR"
   },
   "outputs": [],
   "source": [
    "##Initialise retriever to interact with the Qdrant collection\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXE8pRjjgSse",
    "outputId": "6207af91-b168-4a42-c1de-832246c4a001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ---\n",
      "\n",
      "title: Quantization\n",
      "\n",
      "weight: 120\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../quantization\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Quantization\n",
      "\n",
      "\n",
      "\n",
      "Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n",
      "\n",
      "By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n",
      "\n",
      "Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.\n",
      "\n",
      "2 Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.\n",
      "\n",
      "3 Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n",
      "\n",
      "In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n",
      "\n",
      "\n",
      "\n",
      "There are tradeoffs associated with quantization.\n",
      "\n",
      "On the one hand, quantization allows for significant reductions in storage requirements and faster search times.\n",
      "\n",
      "4 *Available as of v1.1.0*\n",
      "\n",
      "\n",
      "\n",
      "Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n",
      "\n",
      "In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.\n",
      "\n",
      "5 Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(response):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy5llnHLc4lU",
    "outputId": "aa4f5699-9d44-48d9-c24c-fa0cf12e2886"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='df39c370-ba20-4e50-8353-6e58202253ca', embedding=None, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 0}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='351ec4ed-96a0-4ede-98ea-8dba39aaa0c4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 0}, hash='240f864edcd69917078ac2bc6629b1b03ad8c8601ccee13a0e901fab43f94a63'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8bcb0b87-08eb-454d-b3b6-b17d808b31e7', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'documentation/guides/installation.md', 'start_index': 7919}, hash='0b833f3c854a8ab5b29cffcf4a534546f07575e77c51742cc5a47fc20c551961'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='04fb059d-26f2-4f5d-97e2-36159f0745be', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='fe388f2da43760b1a146b3d0ca323ffbd82b3bb259774bf00a532a86a89b6859')}, text='---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.', start_char_idx=0, end_char_idx=481, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8581414),\n",
       " NodeWithScore(node=TextNode(id_='5c7e29e4-6d24-447c-a5ac-5da7977f8b0d', embedding=None, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 1259}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c7fd1901-e9d0-433a-ac17-772ba34e6a76', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 1259}, hash='6995d4c68367c5e2ddc6c6e93e46af14fc2bce5da24c9515aad3a31f9121fdff'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2a08bbdd-25fd-4552-83bf-838a12c7216c', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 1215}, hash='8138e512d7d721a38698c779c5ba62cec91f390a0d8d1a96d6403fb9827296a2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5918d035-6064-4105-8f73-591fa30f0bdd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5019a68c10071f1a30e21328ba697e1ff8dac75f9f38cfa6477ab2b137d7ece3')}, text='Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.', start_char_idx=0, end_char_idx=497, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8141465),\n",
       " NodeWithScore(node=TextNode(id_='04fb059d-26f2-4f5d-97e2-36159f0745be', embedding=None, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 486}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='083afa07-08d5-4284-8b8e-f48507a6514e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 486}, hash='60c64fb3c85c12674923cd33a28c67def3609349cb04e71330f41101cdf86af2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='df39c370-ba20-4e50-8353-6e58202253ca', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 0}, hash='240f864edcd69917078ac2bc6629b1b03ad8c8601ccee13a0e901fab43f94a63'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='53bb45f6-2658-4909-a4c9-deebb1e71f46', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9aa69f4f04b857c3c013c9bbb0c6ce7a3b36660cb36fc3cce7498fd5be1f106e')}, text='Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', start_char_idx=0, end_char_idx=481, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8108046),\n",
       " NodeWithScore(node=TextNode(id_='8ba5a227-0df4-4414-a986-98557bb08378', embedding=None, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 1371}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d0319add-87d3-4331-b322-9a5643179af1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 1371}, hash='af6889450a41260d15ac79ea963e73d4094c7ea79e6abf78da644de4d3c43072'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='53bb45f6-2658-4909-a4c9-deebb1e71f46', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'documentation/guides/quantization.md', 'start_index': 969}, hash='18f7c9a09eff5f53cd903fbcde375e5b74e7e5dfcc9bad2190745e3919dfa79d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4c584d01-50d6-4602-b330-b3c479f4d418', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='12218891a33b9b70a99f2fe59e4165172b6a397d37ebaad79126b4e4b59daa55')}, text='*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.', start_char_idx=0, end_char_idx=473, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.78916),\n",
       " NodeWithScore(node=TextNode(id_='7bd8faa8-7139-4308-b4ce-16880d149fed', embedding=None, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 3124}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5ddb2b35-a4ea-4155-9fa1-25117ac75cd2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 3124}, hash='53351d135d9867161839d7e8edd3f4c0e98d5aa6751dbfa698a07f86187995e7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='cc7efcb3-4685-4f84-b906-a61ce5a6f5f4', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'articles/quantum-quantization.md', 'start_index': 2646}, hash='e9c62ac0673d72591793794036fff145e4c68b2b9d231e1da2dc333b1eda5380'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='56ad3ca0-8b83-4d52-9bec-2831a34dcfe6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ef6b24f8b206760a19838c6dc2fe64830f209bc2b66159d8071d18be8713636c')}, text='Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.', start_char_idx=0, end_char_idx=94, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.77070266)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f7LrVhxuL613",
    "outputId": "397cb8ec-9b3a-44b1-84db-68d75b3bd655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2c6c471c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view the above data in the UI\n",
    "px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfOh7f9VlJMX"
   },
   "source": [
    "### **10. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of the baseline questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "01d4ff90aac943ff83bfe49b70c783ba",
      "8d9844dfef3f414aa750e6200a62bfd3",
      "57d8cc8ff7f047c19301f0793367b2ae",
      "4b23a2eb73b6435384686ced47f9c7f6",
      "6fc5bf4070d94eccbdcb4fad4247c758",
      "218ea3901010405a95033a6cb632250a",
      "8ef997a7dbf54d3d9cfa8147e1611cbd",
      "72ce212a2baa494a8a9ae439c7c3e742",
      "f61b254bd9e24480b0176b43c1c7f47e",
      "d9fa5a2efb1142c9abd47fe46f05cee6",
      "56dc39917eec4eba8194bbe0a3e91de1",
      "c95d4cfffece443182faa3e2a3e48dda",
      "df997aad597e4c1ab23501d12383e39b",
      "a1c3867b1d404a948a0edd0e6bc9a1b8",
      "0c90d55a6f6d4d3f8c72bd0ba19687ba",
      "1e846da2f0c44e099d24c350e0b0eb1f",
      "924c8fe0a9cb4feab39d31ec358089b8",
      "cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "ff9f4b2c32234207acb5221babe9c061",
      "d51b6ff6e830489f8e1e100bf1f1ae98",
      "be68eca0c7b640abada072a3cc3cd042",
      "6eadc3109208449eb9cb7ad5b6a5bb46",
      "1057e11184c44ea38154a2ceabd90198",
      "4722676ae36d4749918f5edaac48b182",
      "a2041da6fb5f499191f405fa891c0907",
      "5c26103d1d89466c9d22960bb347db39",
      "0685ef29e4c54e2e8d5c424508ebfe82",
      "ba3888c2a5954fb29293f41577a5bcae",
      "f7b5096a4b944d62a281ee927a1ce2e4",
      "54aa6aff06214fe4baa3e621ec306c04",
      "66588b1f23bb421db7dbc90b2298a845",
      "4ada07ba27b14c1a8c071343cc21c9c4",
      "98d7cd89900f44a79893e7d8d6b8b0c7"
     ]
    },
    "id": "uFLVc1gkivfp",
    "outputId": "f2e8533d-1524-4365-e150-03d4b8ae811f"
   },
   "outputs": [],
   "source": [
    "## Loading the Eval dataset\n",
    "from datasets import load_dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_doc_qna\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obJfFvhLjs86",
    "outputId": "e634866e-4ca5-491b-e457-feda9afa9d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of oversampling in Qdrant search process?',\n",
       " 'How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?',\n",
       " 'What is the difference between regular and neural search?',\n",
       " 'How can I use Qdrant as a vector store in Langchain Go?',\n",
       " 'How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?',\n",
       " 'Why do we still need keyword search?',\n",
       " 'What principles did Qdrant follow while designing benchmarks for vector search engines?',\n",
       " 'What models does Qdrant support for embedding generation?',\n",
       " 'How can you parallelize the upload of a large dataset using shards in Qdrant?',\n",
       " 'What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzQwf65z3yrz",
    "outputId": "225075d8-855e-47d6-cfc6-0f71d2feedd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:16<00:00,  7.64s/it]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      query_engine.query(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDox2bLJlYO1"
   },
   "source": [
    "Check the Phoenix UI as your queries run. Your traces should appear in real time.\n",
    "\n",
    "Open the Phoenix UI with the link below if you haven't already and click through the queries to better understand how the query engine is performing. For each trace you will see a break\n",
    "\n",
    "Phoenix can be used to understand and troubleshoot your by surfacing:\n",
    " - **Application latency** - highlighting slow invocations of LLMs, Retrievers, etc.\n",
    " - **Token Usage** - Displays the breakdown of token usage with LLMs to surface up your most expensive LLM calls\n",
    " - **Runtime Exceptions** - Critical runtime exceptions such as rate-limiting are captured as exception events.\n",
    " - **Retrieved Documents** - view all the documents retrieved during a retriever call and the score and order in which they were returned\n",
    " - **Embeddings** - view the embedding text used for retrieval and the underlying embedding model\n",
    "LLM Parameters - view the parameters used when calling out to an LLM to debug things like temperature and the system prompts\n",
    " - **Prompt Templates** - Figure out what prompt template is used during the prompting step and what variables were used.\n",
    " - **Tool Descriptions** - view the description and function signature of the tools your LLM has been given access to\n",
    " - **LLM Function Calls** - if using OpenAI or other a model with function calls, you can view the function selection and function messages in the input messages to the LLM.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/RAG_trace_details.png\" alt=\"Trace Details View on Phoenix\" style=\"width:100%; height:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "F-fyd_qP31Xf",
    "outputId": "b33bc66c-f2f1-4363-b14a-e96b85e0f5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_M7evWcldWL"
   },
   "source": [
    "### **11. Export and Evaluate Your Trace Data**\n",
    "You can export your trace data as a pandas dataframe for further analysis and evaluation.\n",
    "\n",
    "In this case, we will export our retriever spans into two separate dataframes:\n",
    "\n",
    "queries_df, in which the retrieved documents for each query are concatenated into a single column, retrieved_documents_df, in which each retrieved document is \"exploded\" into its own row to enable the evaluation of each query-document pair in isolation. This will enable us to compute multiple kinds of evaluations, including:\n",
    "\n",
    "relevance: Are the retrieved documents grounded in the response? Q&A correctness: Are your application's responses grounded in the retrieved context? hallucinations: Is your application making up false information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "gQzwROt6JAf7"
   },
   "outputs": [],
   "source": [
    "queries_df = get_qa_with_reference(px.Client())\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6af1f0cf3b4b91ea</th>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm outputs a set of results that are as dissimilar from each other as possible. This approach is useful for creating a diverse selection from a collection, which can be beneficial in various applications where variety is desired, such as in recommendation systems or when trying to cover a wide range of topics or features within a dataset. By focusing on maximizing distances, the need for manual labeling or categorization is reduced, as the diversity is achieved through the inherent differences in the data points' vector representations.</td>\n",
       "      <td>{{&lt; figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" &gt;}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16bc404cafe31afb</th>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>You can parallelize the upload of a large dataset in Qdrant by creating multiple shards for each collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across them, which allows for parallel data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. To set up a collection with multiple shards, you would specify the \"shard_number\" in your collection creation request. For example, to create a collection with 2 shards, you would include `\"shard_number\": 2` in the PUT request to the `/collections/{collection_name}` endpoint.</td>\n",
       "      <td>## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861c06c418766e7a</th>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package.</td>\n",
       "      <td>. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896e9be994a49f8</th>\n",
       "      <td>What principles did Qdrant follow while designing benchmarks for vector search engines?</td>\n",
       "      <td>The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines, and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.</td>\n",
       "      <td>preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969248e1b4c6205</th>\n",
       "      <td>Why do we still need keyword search?</td>\n",
       "      <td>Keyword-based search was the standard for search engines historically because it was the only option available. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords. These enhancements aimed to make search engines more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.</td>\n",
       "      <td>2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9fe19aa1d622b9a6</th>\n",
       "      <td>How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?</td>\n",
       "      <td>Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.</td>\n",
       "      <td>compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838606c9ae3bc39</th>\n",
       "      <td>How can I use Qdrant as a vector store in Langchain Go?</td>\n",
       "      <td>To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can start integrating Qdrant into your Langchain Go applications. While the context does not provide a specific code example for Langchain Go, you can refer to the documentation at `https://tmc.github.io/langchaingo/docs/` for detailed instructions and usage examples.</td>\n",
       "      <td>---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869ccb19423efe8</th>\n",
       "      <td>What is the difference between regular and neural search?</td>\n",
       "      <td>The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to the query. Neural search, on the other hand, leverages neural networks to understand the context and meaning behind the query, allowing it to find more relevant results even if the exact keywords are not present in the content. This can lead to improved accuracy and a more intuitive search experience, as neural search can interpret the intent and semantic relationships within the data it searches through.</td>\n",
       "      <td>In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e24aea5dff264cce</th>\n",
       "      <td>How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?</td>\n",
       "      <td>Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search. This approach eliminates the need for pre- or post-filtering, which can lead to issues such as a disconnected HNSW graph when too many vectors are filtered out. This modification allows Qdrant to maintain high accuracy and speed in search results. Additionally, Qdrant provides the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.</td>\n",
       "      <td>On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99311e9c03c71f6d</th>\n",
       "      <td>What is the purpose of oversampling in Qdrant search process?</td>\n",
       "      <td>The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for the compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real time by retrieving more vectors than needed from quantized storage and then assigning a more precise score upon re-scoring with the original vectors. From this overselection, only the vectors that are most relevant to the user's query are chosen.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5fa4a67b747eca0</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search alg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                         input  \\\n",
       "context.span_id                                                                                                                                                                  \n",
       "6af1f0cf3b4b91ea                 What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "16bc404cafe31afb                                                                                 How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "861c06c418766e7a                                                                                                     What models does Qdrant support for embedding generation?   \n",
       "8896e9be994a49f8                                                                       What principles did Qdrant follow while designing benchmarks for vector search engines?   \n",
       "8969248e1b4c6205                                                                                                                          Why do we still need keyword search?   \n",
       "9fe19aa1d622b9a6  How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?   \n",
       "7838606c9ae3bc39                                                                                                       How can I use Qdrant as a vector store in Langchain Go?   \n",
       "2869ccb19423efe8                                                                                                     What is the difference between regular and neural search?   \n",
       "e24aea5dff264cce                                                         How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?   \n",
       "99311e9c03c71f6d                                                                                                 What is the purpose of oversampling in Qdrant search process?   \n",
       "b5fa4a67b747eca0                                                                                                                                         What is quantization?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "6af1f0cf3b4b91ea  Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm outputs a set of results that are as dissimilar from each other as possible. This approach is useful for creating a diverse selection from a collection, which can be beneficial in various applications where variety is desired, such as in recommendation systems or when trying to cover a wide range of topics or features within a dataset. By focusing on maximizing distances, the need for manual labeling or categorization is reduced, as the diversity is achieved through the inherent differences in the data points' vector representations.   \n",
       "16bc404cafe31afb                            You can parallelize the upload of a large dataset in Qdrant by creating multiple shards for each collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across them, which allows for parallel data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. To set up a collection with multiple shards, you would specify the \"shard_number\" in your collection creation request. For example, to create a collection with 2 shards, you would include `\"shard_number\": 2` in the PUT request to the `/collections/{collection_name}` endpoint.   \n",
       "861c06c418766e7a                                                                                                                                                                                                                                                                                                                                                                                                 Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package.   \n",
       "8896e9be994a49f8                                                                                                                                       The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines, and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.   \n",
       "8969248e1b4c6205                                                                                                                                                                                                                       Keyword-based search was the standard for search engines historically because it was the only option available. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords. These enhancements aimed to make search engines more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.   \n",
       "9fe19aa1d622b9a6                                                                                                                                                                                                                                                                                                                                                                                                                              Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.   \n",
       "7838606c9ae3bc39                                                                                                                                                                                           To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can start integrating Qdrant into your Langchain Go applications. While the context does not provide a specific code example for Langchain Go, you can refer to the documentation at `https://tmc.github.io/langchaingo/docs/` for detailed instructions and usage examples.   \n",
       "2869ccb19423efe8                                                        The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to the query. Neural search, on the other hand, leverages neural networks to understand the context and meaning behind the query, allowing it to find more relevant results even if the exact keywords are not present in the content. This can lead to improved accuracy and a more intuitive search experience, as neural search can interpret the intent and semantic relationships within the data it searches through.   \n",
       "e24aea5dff264cce                                                                                                                                               Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search. This approach eliminates the need for pre- or post-filtering, which can lead to issues such as a disconnected HNSW graph when too many vectors are filtered out. This modification allows Qdrant to maintain high accuracy and speed in search results. Additionally, Qdrant provides the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.   \n",
       "99311e9c03c71f6d                                                                                          The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for the compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real time by retrieving more vectors than needed from quantized storage and then assigning a more precise score upon re-scoring with the original vectors. From this overselection, only the vectors that are most relevant to the user's query are chosen.   \n",
       "b5fa4a67b747eca0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "6af1f0cf3b4b91ea  {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" >}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...  \n",
       "16bc404cafe31afb                 ## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is  \n",
       "861c06c418766e7a                                                       . So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.  \n",
       "8896e9be994a49f8                                                                                                                                                                                                                                                                                                                                                                                                                           preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.  \n",
       "8969248e1b4c6205                                                                                                                                                                                                                                                               2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.  \n",
       "9fe19aa1d622b9a6  compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...  \n",
       "7838606c9ae3bc39  ---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...  \n",
       "2869ccb19423efe8                                                                                                                                                                       In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?  \n",
       "e24aea5dff264cce                                                                                                                                  On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.  \n",
       "99311e9c03c71f6d                                                                                                                                                                                   ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user  \n",
       "b5fa4a67b747eca0  ---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search alg...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ab6af76a07854549</th>\n",
       "      <th>0</th>\n",
       "      <td>7228c47d158f56ba49d4b3d9f72069ef</td>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>{{&lt; figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" &gt;}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.</td>\n",
       "      <td>0.884858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7228c47d158f56ba49d4b3d9f72069ef</td>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>Diversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix</td>\n",
       "      <td>0.856284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bf5a72b20f7ff529</th>\n",
       "      <th>0</th>\n",
       "      <td>15df98dd17e8ccfdff644291dddb8a71</td>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.881565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15df98dd17e8ccfdff644291dddb8a71</td>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is</td>\n",
       "      <td>0.796063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">35f2880c26e61280</th>\n",
       "      <th>0</th>\n",
       "      <td>ed4015186f097820b266bff8f2d15d30</td>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.</td>\n",
       "      <td>0.845401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed4015186f097820b266bff8f2d15d30</td>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>Since Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.</td>\n",
       "      <td>0.832906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8d1fe45a23c35db7</th>\n",
       "      <th>0</th>\n",
       "      <td>447a7f7dd1599ca09ffd51825d2ad83c</td>\n",
       "      <td>What principles did Qdrant follow while designing benchmarks for vector search engines?</td>\n",
       "      <td>preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---</td>\n",
       "      <td>0.857022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>447a7f7dd1599ca09ffd51825d2ad83c</td>\n",
       "      <td>What principles did Qdrant follow while designing benchmarks for vector search engines?</td>\n",
       "      <td>. In this article, we will compare how Qdrant performs against the other vector search engines.</td>\n",
       "      <td>0.853702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">c2e8c0ed7ba9d913</th>\n",
       "      <th>0</th>\n",
       "      <td>a7fb45e4eaa6c26721257639531a9393</td>\n",
       "      <td>Why do we still need keyword search?</td>\n",
       "      <td>2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional</td>\n",
       "      <td>0.795507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a7fb45e4eaa6c26721257639531a9393</td>\n",
       "      <td>Why do we still need keyword search?</td>\n",
       "      <td>. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.</td>\n",
       "      <td>0.762174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4061dfa2c004e163</th>\n",
       "      <th>0</th>\n",
       "      <td>ec792dffd49edf5362b65aaee74ca4aa</td>\n",
       "      <td>How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?</td>\n",
       "      <td>compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.</td>\n",
       "      <td>0.903856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec792dffd49edf5362b65aaee74ca4aa</td>\n",
       "      <td>How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?</td>\n",
       "      <td>![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)</td>\n",
       "      <td>0.857061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e22c10405cebfe25</th>\n",
       "      <th>0</th>\n",
       "      <td>e27fe3a466a72e46d6e3c0ec204a2ba2</td>\n",
       "      <td>How can I use Qdrant as a vector store in Langchain Go?</td>\n",
       "      <td>---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:</td>\n",
       "      <td>0.878504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e27fe3a466a72e46d6e3c0ec204a2ba2</td>\n",
       "      <td>How can I use Qdrant as a vector store in Langchain Go?</td>\n",
       "      <td>```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model_name=\"sentence-transformers/all-mpnet-base-v2\"\\n\\n)</td>\n",
       "      <td>0.861317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">586bc3130bdfd530</th>\n",
       "      <th>0</th>\n",
       "      <td>15cd80946ede20e0f147f1644fc04137</td>\n",
       "      <td>What is the difference between regular and neural search?</td>\n",
       "      <td>In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**</td>\n",
       "      <td>0.861072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15cd80946ede20e0f147f1644fc04137</td>\n",
       "      <td>What is the difference between regular and neural search?</td>\n",
       "      <td>From web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?</td>\n",
       "      <td>0.837844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">aea93c3acb7a6246</th>\n",
       "      <th>0</th>\n",
       "      <td>d303fe1a2b610a5e025d3dc96eba0e1e</td>\n",
       "      <td>How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?</td>\n",
       "      <td>On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.</td>\n",
       "      <td>0.857037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d303fe1a2b610a5e025d3dc96eba0e1e</td>\n",
       "      <td>How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?</td>\n",
       "      <td>HNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.</td>\n",
       "      <td>0.832162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">be315cc98bd1f061</th>\n",
       "      <th>0</th>\n",
       "      <td>7c6aeecd64cde5939886946763f3f4bb</td>\n",
       "      <td>What is the purpose of oversampling in Qdrant search process?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.871287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7c6aeecd64cde5939886946763f3f4bb</td>\n",
       "      <td>What is the purpose of oversampling in Qdrant search process?</td>\n",
       "      <td>Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user</td>\n",
       "      <td>0.845119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">b5fa4a67b747eca0</th>\n",
       "      <th>0</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.</td>\n",
       "      <td>0.858141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.</td>\n",
       "      <td>0.814146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.</td>\n",
       "      <td>0.810805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.789160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.</td>\n",
       "      <td>0.770703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "ab6af76a07854549 0                  7228c47d158f56ba49d4b3d9f72069ef   \n",
       "                 1                  7228c47d158f56ba49d4b3d9f72069ef   \n",
       "bf5a72b20f7ff529 0                  15df98dd17e8ccfdff644291dddb8a71   \n",
       "                 1                  15df98dd17e8ccfdff644291dddb8a71   \n",
       "35f2880c26e61280 0                  ed4015186f097820b266bff8f2d15d30   \n",
       "                 1                  ed4015186f097820b266bff8f2d15d30   \n",
       "8d1fe45a23c35db7 0                  447a7f7dd1599ca09ffd51825d2ad83c   \n",
       "                 1                  447a7f7dd1599ca09ffd51825d2ad83c   \n",
       "c2e8c0ed7ba9d913 0                  a7fb45e4eaa6c26721257639531a9393   \n",
       "                 1                  a7fb45e4eaa6c26721257639531a9393   \n",
       "4061dfa2c004e163 0                  ec792dffd49edf5362b65aaee74ca4aa   \n",
       "                 1                  ec792dffd49edf5362b65aaee74ca4aa   \n",
       "e22c10405cebfe25 0                  e27fe3a466a72e46d6e3c0ec204a2ba2   \n",
       "                 1                  e27fe3a466a72e46d6e3c0ec204a2ba2   \n",
       "586bc3130bdfd530 0                  15cd80946ede20e0f147f1644fc04137   \n",
       "                 1                  15cd80946ede20e0f147f1644fc04137   \n",
       "aea93c3acb7a6246 0                  d303fe1a2b610a5e025d3dc96eba0e1e   \n",
       "                 1                  d303fe1a2b610a5e025d3dc96eba0e1e   \n",
       "be315cc98bd1f061 0                  7c6aeecd64cde5939886946763f3f4bb   \n",
       "                 1                  7c6aeecd64cde5939886946763f3f4bb   \n",
       "b5fa4a67b747eca0 0                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 1                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 2                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 3                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 4                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "\n",
       "                                                                                                                                                                                           input  \\\n",
       "context.span_id  document_position                                                                                                                                                                 \n",
       "ab6af76a07854549 0                                 What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "                 1                                 What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "bf5a72b20f7ff529 0                                                                                                 How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "                 1                                                                                                 How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "35f2880c26e61280 0                                                                                                                     What models does Qdrant support for embedding generation?   \n",
       "                 1                                                                                                                     What models does Qdrant support for embedding generation?   \n",
       "8d1fe45a23c35db7 0                                                                                       What principles did Qdrant follow while designing benchmarks for vector search engines?   \n",
       "                 1                                                                                       What principles did Qdrant follow while designing benchmarks for vector search engines?   \n",
       "c2e8c0ed7ba9d913 0                                                                                                                                          Why do we still need keyword search?   \n",
       "                 1                                                                                                                                          Why do we still need keyword search?   \n",
       "4061dfa2c004e163 0                  How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?   \n",
       "                 1                  How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?   \n",
       "e22c10405cebfe25 0                                                                                                                       How can I use Qdrant as a vector store in Langchain Go?   \n",
       "                 1                                                                                                                       How can I use Qdrant as a vector store in Langchain Go?   \n",
       "586bc3130bdfd530 0                                                                                                                     What is the difference between regular and neural search?   \n",
       "                 1                                                                                                                     What is the difference between regular and neural search?   \n",
       "aea93c3acb7a6246 0                                                                         How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?   \n",
       "                 1                                                                         How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?   \n",
       "be315cc98bd1f061 0                                                                                                                 What is the purpose of oversampling in Qdrant search process?   \n",
       "                 1                                                                                                                 What is the purpose of oversampling in Qdrant search process?   \n",
       "b5fa4a67b747eca0 0                                                                                                                                                         What is quantization?   \n",
       "                 1                                                                                                                                                         What is quantization?   \n",
       "                 2                                                                                                                                                         What is quantization?   \n",
       "                 3                                                                                                                                                         What is quantization?   \n",
       "                 4                                                                                                                                                         What is quantization?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "ab6af76a07854549 0                                                                              {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" >}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.   \n",
       "                 1                          Diversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix   \n",
       "bf5a72b20f7ff529 0                             ## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "                 1                                                                                             setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is   \n",
       "35f2880c26e61280 0                                                                                                                    . So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.   \n",
       "                 1                                            Since Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.   \n",
       "8d1fe45a23c35db7 0                                                            preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---   \n",
       "                 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                        . In this article, we will compare how Qdrant performs against the other vector search engines.   \n",
       "c2e8c0ed7ba9d913 0                                                                                                                       2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional   \n",
       "                 1                                                                                                                                                                                                                                                 . We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.   \n",
       "4061dfa2c004e163 0                                     compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.   \n",
       "                 1                                               ![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)   \n",
       "e22c10405cebfe25 0                  ---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:   \n",
       "                 1                                        ```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model_name=\"sentence-transformers/all-mpnet-base-v2\"\\n\\n)   \n",
       "586bc3130bdfd530 0                                                                                                                                                                                                                    In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**   \n",
       "                 1                                                            From web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?   \n",
       "aea93c3acb7a6246 0                                                                                                                                                                                        On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.   \n",
       "                 1                                                   HNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.   \n",
       "be315cc98bd1f061 0                                                                                                                                               ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 1                                                                                                                                             Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user   \n",
       "b5fa4a67b747eca0 0                                                ---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.   \n",
       "                 1                                                      Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.   \n",
       "                 2                                                              Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.   \n",
       "                 3                                                                  *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "                 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "ab6af76a07854549 0                        0.884858  \n",
       "                 1                        0.856284  \n",
       "bf5a72b20f7ff529 0                        0.881565  \n",
       "                 1                        0.796063  \n",
       "35f2880c26e61280 0                        0.845401  \n",
       "                 1                        0.832906  \n",
       "8d1fe45a23c35db7 0                        0.857022  \n",
       "                 1                        0.853702  \n",
       "c2e8c0ed7ba9d913 0                        0.795507  \n",
       "                 1                        0.762174  \n",
       "4061dfa2c004e163 0                        0.903856  \n",
       "                 1                        0.857061  \n",
       "e22c10405cebfe25 0                        0.878504  \n",
       "                 1                        0.861317  \n",
       "586bc3130bdfd530 0                        0.861072  \n",
       "                 1                        0.837844  \n",
       "aea93c3acb7a6246 0                        0.857037  \n",
       "                 1                        0.832162  \n",
       "be315cc98bd1f061 0                        0.871287  \n",
       "                 1                        0.845119  \n",
       "b5fa4a67b747eca0 0                        0.858141  \n",
       "                 1                        0.814146  \n",
       "                 2                        0.810805  \n",
       "                 3                        0.789160  \n",
       "                 4                        0.770703  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Define your evaluation model and your evaluators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4XwhUYRlkdP"
   },
   "source": [
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7b8b3e1c571a4345bc2ec4eb00f0967e",
      "a74d6dce715f4494874de92852b56c09",
      "a5d80d3579ca481a98310bd08b8c6918",
      "f07a59b5bc7a4f56baa892ca864d488c",
      "84fd76a737334831a886755affb50886",
      "5c65981c04f34589a36c5c8bb4195f2b",
      "10d5d30b4fd14f3ba45da1794607c5e0",
      "ce0efd7352cd4539bda8706ebbc146fd",
      "128b89e88c684401818304e8410b6c33",
      "e7d4e009f37a4d798dbc23b39ed7c0b1",
      "d772ecb46ca54adbbb84749fe07ab735",
      "68ac22f645934827a9627e3559067ba1",
      "f3c5e97c2ac94cafa20b130f1f45ce67",
      "b505d5e7978d4b19b59dc8f3fe71e795",
      "080a6d0c05144698a9c7366e54bb39b0",
      "e69da775e4df4ed8a5f5ed1348278ab8",
      "77d79ac84c8c4d8393fcdb7f304b4bf3",
      "43dfd4070e324858974c08fbcb8055fd",
      "38d701e482684695b97bd73bac78ccfc",
      "e7d21aa7bce145b78e834897b158f9b0",
      "65f86e93b0fe48ddbaf2cabc9975c68b",
      "8e261a6071cd41a6afd4377c9ead98ca"
     ]
    },
    "id": "SnJSWjOkJGpE",
    "outputId": "b7119003-dd01-4a37-da4f-ed2bf3f130d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4681ac86a5d340a7a22ab0fe6356cc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/22 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f731124261843bab331619e6cf6f904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOWquDbAlm4P"
   },
   "source": [
    "Your evaluations should now appear as annotations on the appropriate spans in Phoenix.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5yek2mtlrRA"
   },
   "source": [
    "### **13. Let's try Hybrid search now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a new collection to store your hybrid emebeddings\n",
    "COLLECTION_NAME_HYBRID = \"qdrant_docs_arize_hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reprocess documents with different settings if needed \n",
    "#documents = process_document_chunks(dataset , CHUNK_SIZE , CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n",
       " {'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##List of supported sparse vector models\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding\n",
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Ingest Sparse and Dense vectors into Qdrant**\n",
    "\n",
    "Ingest sparse and dense vectors into Qdrant Collection.\n",
    "We are using Splade++ model for Sparse Vector Model and default Fastembed model - bge-small-en-1.5 for dense embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bced3890a34890a0928da03ce19924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08558ed2c4149d684f8acbe11597b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/133 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8925b1d73efa4e0ab1277131178681e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb8b7ae9be404aac0fb52471e1c3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8a8cfe7f7d4763a0a8f5ae2a04756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4e6145b5ec41c5a9241cecea2738bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4324423aa19408ea25c37c1f512159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559d4058b73f42fbb03679a7b765ac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding, SparseEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from typing import List, Tuple\n",
    "\n",
    "sparse_model_name = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "\n",
    "batch_size = 10\n",
    "parallel = 0\n",
    "\n",
    "## Computing sparse vectors\n",
    "def compute_sparse_vectors(\n",
    "    texts: List[str],\n",
    "    ) -> Tuple[List[List[int]], List[List[float]]]:\n",
    "    indices, values = [], []\n",
    "    for embedding in sparse_model.embed(texts):\n",
    "        indices.append(embedding.indices.tolist())\n",
    "        values.append(embedding.values.tolist())\n",
    "    return indices, values\n",
    "\n",
    "## Creating a vector store with Hybrid search enabled\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    sparse_doc_fn=compute_sparse_vectors,\n",
    "    sparse_query_fn=compute_sparse_vectors)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Ingesting sparse and dense vectors into Qdrant collection\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=8862, indexed_vectors_count=4429, points_count=4431, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors={'text-dense': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'text-sparse': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None))}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collection level operations\n",
    "client.get_collection(COLLECTION_NAME_HYBRID)\n",
    "#client.delete_collection(COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the number of documents matches the expected number of document chunks \n",
    "client.count(collection_name=COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. Hybrid Search with Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise Hybrid Vector Store \n",
    "vector_store_hybrid = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,  # this is important for the ingestion\n",
    ")\n",
    "\n",
    "## Followed by initialising index for interacting with the Hybrid Collection in Qdrant\n",
    "\n",
    "hybrid_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store_hybrid,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/atitaarora/.zshenv:2: no such file or directory: /Users/atitaarora/qdrant/workspace/qdrant-rag-eval/workshop-rag-eval-qdrant-arize/arize-eval/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Users/atitaarora/.cargo/bin=/Users/atitaarora/qdrant/workspace/qdrant-rag-eval/workshop-rag-eval-qdrant-arize/arize-eval/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Users/atitaarora/.cargo/bin+,,/Users/atitaarora/Library/Python/3.10/bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.39.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO add this to poetry\n",
    "#!pip install \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n",
      "\n",
      "In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n",
      "\n",
      "\n",
      "\n",
      "There are tradeoffs associated with quantization.\n",
      "\n",
      "On the one hand, quantization allows for significant reductions in storage requirements and faster search times.\n",
      "\n",
      "2 ---\n",
      "\n",
      "title: Quantization\n",
      "\n",
      "weight: 120\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../quantization\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Quantization\n",
      "\n",
      "\n",
      "\n",
      "Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n",
      "\n",
      "By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n",
      "\n",
      "Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Before moving further , lets try Sparse Vector Search Retreiver \n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "sparse_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.SPARSE,\n",
    "    sparse_top_k=2,\n",
    ")\n",
    "\n",
    "## Pure sparse vector search\n",
    "nodes = sparse_retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n",
      "\n",
      "In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n",
      "\n",
      "\n",
      "\n",
      "There are tradeoffs associated with quantization.\n",
      "\n",
      "On the one hand, quantization allows for significant reductions in storage requirements and faster search times.\n",
      "\n",
      "2 ---\n",
      "\n",
      "title: Quantization\n",
      "\n",
      "weight: 120\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../quantization\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Quantization\n",
      "\n",
      "\n",
      "\n",
      "Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n",
      "\n",
      "By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n",
      "\n",
      "Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.\n",
      "\n",
      "3 Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.\n",
      "\n",
      "4 *Available as of v1.1.0*\n",
      "\n",
      "\n",
      "\n",
      "Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n",
      "\n",
      "In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.\n",
      "\n",
      "5 Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Let's try Hybrid Search Retriever now\n",
    "hybrid_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    sparse_top_k=2,\n",
    "    similarity_top_k=5,\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "nodes = hybrid_retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ---\n",
      "\n",
      "title: Quantization\n",
      "\n",
      "weight: 120\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../quantization\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Quantization\n",
      "\n",
      "\n",
      "\n",
      "Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n",
      "\n",
      "By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n",
      "\n",
      "Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.\n",
      "\n",
      "2 Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n",
      "\n",
      "In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n",
      "\n",
      "\n",
      "\n",
      "There are tradeoffs associated with quantization.\n",
      "\n",
      "On the one hand, quantization allows for significant reductions in storage requirements and faster search times.\n",
      "\n",
      "3 Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.\n",
      "\n",
      "4 *Available as of v1.1.0*\n",
      "\n",
      "\n",
      "\n",
      "Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n",
      "\n",
      "In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.\n",
      "\n",
      "5 Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We shouldn't be modifying the alpha parameter after the retriever has been created\n",
    "# but that's the easiest way to show the effect of the parameter\n",
    "#hybrid_retriever._alpha = 0.1\n",
    "hybrid_retriever._alpha = 0.9\n",
    "\n",
    "nodes = hybrid_retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Re-Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "Let's rerun the list of the baseline questions about Qdrant on the Hybrid Retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                        | 1/10 [00:06<01:02,  6.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                           | 2/10 [00:10<00:41,  5.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                             | 3/10 [00:17<00:39,  5.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                | 4/10 [00:22<00:33,  5.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 5/10 [00:25<00:23,  4.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 6/10 [00:29<00:18,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 7/10 [00:34<00:13,  4.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 8/10 [00:37<00:08,  4.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 9/10 [00:44<00:05,  5.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:50<00:00,  5.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "# all spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "\n",
    "    ##Reuse the previously loaded dataset `qdrant_qa_question`\n",
    "\n",
    "    query_engine_hybrid = hybrid_index.as_query_engine()\n",
    "    for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "        try:\n",
    "          query_engine_hybrid.query(query)\n",
    "        except Exception as e:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "    queries_df_hybrid = get_qa_with_reference(px.Client())\n",
    "    retrieved_documents_df_hybrid = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15e565515ffd3b11</th>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm selects a set of results that are as dissimilar from each other as possible. This approach is used to create a diverse selection from a collection, which is particularly useful when the goal is to represent the broadest possible range of data without redundancy. By focusing on maximizing distances, the algorithm avoids clustering around similar points and instead provides a more varied and representative sample of the dataset. This is beneficial in scenarios where diversity is desired, such as in recommendation systems, content discovery, or when trying to understand the full scope of a dataset without the need for manual labeling.</td>\n",
       "      <td>{{&lt; figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" &gt;}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6033f6352905182</th>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>To parallelize the upload of a large dataset using shards in Qdrant, you can create multiple shards within a collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across these shards, allowing for concurrent data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. When setting up your collection, you can specify the number of shards using the \"shard_number\" parameter in your collection creation request. Here is an example of how to set the shard number to 2 in a collection creation request:\\n\\n```http\\nPUT /collections/{collection_name}\\n{\\n    \"vectors\": {\\n      \"size\": 768,\\n      \"distance\": \"Cosine\"\\n    },\\n    \"shard_number\": 2\\n}\\n```</td>\n",
       "      <td>## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091e771fd80e9b7</th>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package. An ONNX version of the All-MiniLM-L6-V2 model is also mentioned as being used within the service.</td>\n",
       "      <td>. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be59a382b2fbca25</th>\n",
       "      <td>What principles did Qdrant follow while designing benchmarks for vector search engines?</td>\n",
       "      <td>The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.</td>\n",
       "      <td>preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072852478af9880</th>\n",
       "      <td>Why do we still need keyword search?</td>\n",
       "      <td>Keyword-based search was the standard for search engines historically because it was the only option available at the time. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords to make the search process more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.</td>\n",
       "      <td>2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15428eaf64fa2be9</th>\n",
       "      <td>How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?</td>\n",
       "      <td>Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.</td>\n",
       "      <td>compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ec4818782c0c9b2</th>\n",
       "      <td>How can I use Qdrant as a vector store in Langchain Go?</td>\n",
       "      <td>To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can integrate Qdrant into your Langchain Go application. While the context does not provide a specific code example for using Qdrant with Langchain Go, you would typically need to import the necessary packages and configure Qdrant as your vector store within your Go application.</td>\n",
       "      <td>---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fabee3aeeb75ea50</th>\n",
       "      <td>What is the difference between regular and neural search?</td>\n",
       "      <td>The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to those in the query. Neural search, on the other hand, leverages neural networks to understand the context and semantics of the query, providing more relevant results by interpreting the intent behind the search terms rather than relying solely on keyword matching.</td>\n",
       "      <td>In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82f0ed3dffe39306</th>\n",
       "      <td>How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?</td>\n",
       "      <td>Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search without requiring pre- or post-filtering. This approach allows Qdrant to maintain high accuracy and speed, as evidenced by public benchmarks. Additionally, Qdrant offers the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.</td>\n",
       "      <td>On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbf315dd14863caf</th>\n",
       "      <td>What is the purpose of oversampling in Qdrant search process?</td>\n",
       "      <td>The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for significant compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real-time, by retrieving more vectors than needed from quantized storage and then assigning a more precise score during re-scoring. From this overselection, only the vectors that are most relevant to the user are chosen.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c573b0264e4e74b0</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4f61211ac2ac2c8a</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927a47b44ffd309</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578595e1e98482e4</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c558029651ccc8a2</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60cbd0860cebd22f</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ee122cc109643164</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6af1f0cf3b4b91ea</th>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm outputs a set of results that are as dissimilar from each other as possible. This approach is useful for creating a diverse selection from a collection, which can be beneficial in various applications where variety is desired, such as in recommendation systems or when trying to cover a wide range of topics or features within a dataset. By focusing on maximizing distances, the need for manual labeling or categorization is reduced, as the diversity is achieved through the inherent differences in the data points' vector representations.</td>\n",
       "      <td>{{&lt; figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" &gt;}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16bc404cafe31afb</th>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>You can parallelize the upload of a large dataset in Qdrant by creating multiple shards for each collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across them, which allows for parallel data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. To set up a collection with multiple shards, you would specify the \"shard_number\" in your collection creation request. For example, to create a collection with 2 shards, you would include `\"shard_number\": 2` in the PUT request to the `/collections/{collection_name}` endpoint.</td>\n",
       "      <td>## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861c06c418766e7a</th>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package.</td>\n",
       "      <td>. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896e9be994a49f8</th>\n",
       "      <td>What principles did Qdrant follow while designing benchmarks for vector search engines?</td>\n",
       "      <td>The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines, and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.</td>\n",
       "      <td>preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969248e1b4c6205</th>\n",
       "      <td>Why do we still need keyword search?</td>\n",
       "      <td>Keyword-based search was the standard for search engines historically because it was the only option available. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords. These enhancements aimed to make search engines more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.</td>\n",
       "      <td>2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9fe19aa1d622b9a6</th>\n",
       "      <td>How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?</td>\n",
       "      <td>Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.</td>\n",
       "      <td>compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838606c9ae3bc39</th>\n",
       "      <td>How can I use Qdrant as a vector store in Langchain Go?</td>\n",
       "      <td>To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can start integrating Qdrant into your Langchain Go applications. While the context does not provide a specific code example for Langchain Go, you can refer to the documentation at `https://tmc.github.io/langchaingo/docs/` for detailed instructions and usage examples.</td>\n",
       "      <td>---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869ccb19423efe8</th>\n",
       "      <td>What is the difference between regular and neural search?</td>\n",
       "      <td>The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to the query. Neural search, on the other hand, leverages neural networks to understand the context and meaning behind the query, allowing it to find more relevant results even if the exact keywords are not present in the content. This can lead to improved accuracy and a more intuitive search experience, as neural search can interpret the intent and semantic relationships within the data it searches through.</td>\n",
       "      <td>In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e24aea5dff264cce</th>\n",
       "      <td>How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?</td>\n",
       "      <td>Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search. This approach eliminates the need for pre- or post-filtering, which can lead to issues such as a disconnected HNSW graph when too many vectors are filtered out. This modification allows Qdrant to maintain high accuracy and speed in search results. Additionally, Qdrant provides the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.</td>\n",
       "      <td>On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99311e9c03c71f6d</th>\n",
       "      <td>What is the purpose of oversampling in Qdrant search process?</td>\n",
       "      <td>The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for the compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real time by retrieving more vectors than needed from quantized storage and then assigning a more precise score upon re-scoring with the original vectors. From this overselection, only the vectors that are most relevant to the user's query are chosen.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5fa4a67b747eca0</th>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>None</td>\n",
       "      <td>---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search alg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                         input  \\\n",
       "context.span_id                                                                                                                                                                  \n",
       "15e565515ffd3b11                 What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "d6033f6352905182                                                                                 How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "1091e771fd80e9b7                                                                                                     What models does Qdrant support for embedding generation?   \n",
       "be59a382b2fbca25                                                                       What principles did Qdrant follow while designing benchmarks for vector search engines?   \n",
       "5072852478af9880                                                                                                                          Why do we still need keyword search?   \n",
       "15428eaf64fa2be9  How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?   \n",
       "2ec4818782c0c9b2                                                                                                       How can I use Qdrant as a vector store in Langchain Go?   \n",
       "fabee3aeeb75ea50                                                                                                     What is the difference between regular and neural search?   \n",
       "82f0ed3dffe39306                                                         How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?   \n",
       "cbf315dd14863caf                                                                                                 What is the purpose of oversampling in Qdrant search process?   \n",
       "c573b0264e4e74b0                                                                                                                                         What is quantization?   \n",
       "4f61211ac2ac2c8a                                                                                                                                         What is quantization?   \n",
       "1927a47b44ffd309                                                                                                                                         What is quantization?   \n",
       "578595e1e98482e4                                                                                                                                         What is quantization?   \n",
       "c558029651ccc8a2                                                                                                                                         What is quantization?   \n",
       "60cbd0860cebd22f                                                                                                                                         What is quantization?   \n",
       "ee122cc109643164                                                                                                                                         What is quantization?   \n",
       "6af1f0cf3b4b91ea                 What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "16bc404cafe31afb                                                                                 How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "861c06c418766e7a                                                                                                     What models does Qdrant support for embedding generation?   \n",
       "8896e9be994a49f8                                                                       What principles did Qdrant follow while designing benchmarks for vector search engines?   \n",
       "8969248e1b4c6205                                                                                                                          Why do we still need keyword search?   \n",
       "9fe19aa1d622b9a6  How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?   \n",
       "7838606c9ae3bc39                                                                                                       How can I use Qdrant as a vector store in Langchain Go?   \n",
       "2869ccb19423efe8                                                                                                     What is the difference between regular and neural search?   \n",
       "e24aea5dff264cce                                                         How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?   \n",
       "99311e9c03c71f6d                                                                                                 What is the purpose of oversampling in Qdrant search process?   \n",
       "b5fa4a67b747eca0                                                                                                                                         What is quantization?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "15e565515ffd3b11   Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm selects a set of results that are as dissimilar from each other as possible. This approach is used to create a diverse selection from a collection, which is particularly useful when the goal is to represent the broadest possible range of data without redundancy. By focusing on maximizing distances, the algorithm avoids clustering around similar points and instead provides a more varied and representative sample of the dataset. This is beneficial in scenarios where diversity is desired, such as in recommendation systems, content discovery, or when trying to understand the full scope of a dataset without the need for manual labeling.   \n",
       "d6033f6352905182  To parallelize the upload of a large dataset using shards in Qdrant, you can create multiple shards within a collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across these shards, allowing for concurrent data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. When setting up your collection, you can specify the number of shards using the \"shard_number\" parameter in your collection creation request. Here is an example of how to set the shard number to 2 in a collection creation request:\\n\\n```http\\nPUT /collections/{collection_name}\\n{\\n    \"vectors\": {\\n      \"size\": 768,\\n      \"distance\": \"Cosine\"\\n    },\\n    \"shard_number\": 2\\n}\\n```   \n",
       "1091e771fd80e9b7                                                                                                                                                                                                                                                                                                                                                                                                    Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package. An ONNX version of the All-MiniLM-L6-V2 model is also mentioned as being used within the service.   \n",
       "be59a382b2fbca25                                                                                                                                                                                                                                             The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.   \n",
       "5072852478af9880                                                                                                                                                                                                                                                                                                                                      Keyword-based search was the standard for search engines historically because it was the only option available at the time. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords to make the search process more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.   \n",
       "15428eaf64fa2be9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.   \n",
       "2ec4818782c0c9b2                                                                                                                                                                                                                                                                                     To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can integrate Qdrant into your Langchain Go application. While the context does not provide a specific code example for using Qdrant with Langchain Go, you would typically need to import the necessary packages and configure Qdrant as your vector store within your Go application.   \n",
       "fabee3aeeb75ea50                                                                                                                                                                                                                                                                                                             The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to those in the query. Neural search, on the other hand, leverages neural networks to understand the context and semantics of the query, providing more relevant results by interpreting the intent behind the search terms rather than relying solely on keyword matching.   \n",
       "82f0ed3dffe39306                                                                                                                                                                                                                                                                                                                                                                 Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search without requiring pre- or post-filtering. This approach allows Qdrant to maintain high accuracy and speed, as evidenced by public benchmarks. Additionally, Qdrant offers the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.   \n",
       "cbf315dd14863caf                                                                                                                                                                                                                      The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for significant compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real-time, by retrieving more vectors than needed from quantized storage and then assigning a more precise score during re-scoring. From this overselection, only the vectors that are most relevant to the user are chosen.   \n",
       "c573b0264e4e74b0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "4f61211ac2ac2c8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "1927a47b44ffd309                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "578595e1e98482e4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "c558029651ccc8a2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "60cbd0860cebd22f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "ee122cc109643164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "6af1f0cf3b4b91ea                                                                                                       Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it ensures that the algorithm outputs a set of results that are as dissimilar from each other as possible. This approach is useful for creating a diverse selection from a collection, which can be beneficial in various applications where variety is desired, such as in recommendation systems or when trying to cover a wide range of topics or features within a dataset. By focusing on maximizing distances, the need for manual labeling or categorization is reduced, as the diversity is achieved through the inherent differences in the data points' vector representations.   \n",
       "16bc404cafe31afb                                                                                                                                 You can parallelize the upload of a large dataset in Qdrant by creating multiple shards for each collection. Each shard operates with its own Write-Ahead-Log (WAL), which is responsible for ordering operations. By having multiple shards, you can distribute the upload process across them, which allows for parallel data ingestion. A reasonable number of shards to create per machine ranges from 2 to 4. To set up a collection with multiple shards, you would specify the \"shard_number\" in your collection creation request. For example, to create a collection with 2 shards, you would include `\"shard_number\": 2` in the PUT request to the `/collections/{collection_name}` endpoint.   \n",
       "861c06c418766e7a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Qdrant does not support embedding generation by itself; it requires external models to generate embeddings before they can be stored in the vector database. The models mentioned for embedding generation are the Mpnet model, various multilingual models, and the All-MiniLM-L6-V2 model from the SentenceTransformers package.   \n",
       "8896e9be994a49f8                                                                                                                                                                                                                                            The principles that Qdrant followed while designing benchmarks for vector search engines are not explicitly stated in the provided context information. The context only indicates that Qdrant has created the first comparative benchmark and benchmarking framework for vector search engines, and that an article will compare Qdrant's performance against other vector search engines. To understand the specific principles Qdrant followed, one would need to refer to additional information or documentation that details their benchmarking methodology and design considerations.   \n",
       "8969248e1b4c6205                                                                                                                                                                                                                                                                                                                            Keyword-based search was the standard for search engines historically because it was the only option available. Despite facing common issues, efforts were made to improve its effectiveness by implementing strategies such as converting words into their root forms and removing stopwords. These enhancements aimed to make search engines more user-friendly. The fundamental concept of keyword search has been around since the Middle Ages and has been refined over time to better meet user needs.   \n",
       "9fe19aa1d622b9a6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Dust leveraged the control of the MMAP payload threshold and Scalar Quantization to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively. This approach allowed them to scale smoothly and maintain good performance even when RAM was fully utilized.   \n",
       "7838606c9ae3bc39                                                                                                                                                                                                                                                                                                To use Qdrant as a vector store in Langchain Go, you need to install the `langchain-go` project dependency by running the following command in your terminal:\\n\\n```bash\\ngo get -u github.com/tmc/langchaingo\\n```\\n\\nAfter installing the dependency, you can start integrating Qdrant into your Langchain Go applications. While the context does not provide a specific code example for Langchain Go, you can refer to the documentation at `https://tmc.github.io/langchaingo/docs/` for detailed instructions and usage examples.   \n",
       "2869ccb19423efe8                                                                                                                                                             The difference between regular and neural search lies in the underlying technology used to perform the search. Regular search typically uses keyword matching and traditional algorithms to find results that contain the same or similar words to the query. Neural search, on the other hand, leverages neural networks to understand the context and meaning behind the query, allowing it to find more relevant results even if the exact keywords are not present in the content. This can lead to improved accuracy and a more intuitive search experience, as neural search can interpret the intent and semantic relationships within the data it searches through.   \n",
       "e24aea5dff264cce                                                                                                                                                                                                                                                    Qdrant addresses the search accuracy problem by using a modified version of the HNSW algorithm that is compatible with the use of filters during a search. This approach eliminates the need for pre- or post-filtering, which can lead to issues such as a disconnected HNSW graph when too many vectors are filtered out. This modification allows Qdrant to maintain high accuracy and speed in search results. Additionally, Qdrant provides the ability to configure HNSW parameters on a collection and named vector level, which can be used to fine-tune search performance.   \n",
       "99311e9c03c71f6d                                                                                                                                                                                               The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms. It allows for the compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique is used to control the precision of the search in real time by retrieving more vectors than needed from quantized storage and then assigning a more precise score upon re-scoring with the original vectors. From this overselection, only the vectors that are most relevant to the user's query are chosen.   \n",
       "b5fa4a67b747eca0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "15e565515ffd3b11  {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" >}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...  \n",
       "d6033f6352905182                 ## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is  \n",
       "1091e771fd80e9b7                                                       . So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.  \n",
       "be59a382b2fbca25                                                                                                                                                                                                                                                                                                                                                                                                                           preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.  \n",
       "5072852478af9880                                                                                                                                                                                                                                                               2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.  \n",
       "15428eaf64fa2be9  compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...  \n",
       "2ec4818782c0c9b2  ---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...  \n",
       "fabee3aeeb75ea50                                                                                                                                                                       In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?  \n",
       "82f0ed3dffe39306                                                                                                                                  On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.  \n",
       "cbf315dd14863caf                                                                                                                                                                                   ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user  \n",
       "c573b0264e4e74b0  Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....  \n",
       "4f61211ac2ac2c8a     Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.  \n",
       "1927a47b44ffd309  ---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times....  \n",
       "578595e1e98482e4  Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....  \n",
       "c558029651ccc8a2     Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.  \n",
       "60cbd0860cebd22f  Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....  \n",
       "ee122cc109643164  Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.\\n\\n---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section....  \n",
       "6af1f0cf3b4b91ea  {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" >}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\\n\\nDiversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance ...  \n",
       "16bc404cafe31afb                 ## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nsetup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is  \n",
       "861c06c418766e7a                                                       . So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.\\n\\nSince Qdrant doesn't embed by itself, I had to decide on an embedding model. The prior version used the [SentenceTransformers](https://www.sbert.net/) package, which in turn employs Bert-based [All-MiniLM-L6-V2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main) model. This model is battle-tested and delivers fair results at speed, so not experimenting on this front I took an [ONNX version](https://huggingface.co/optimum/all-MiniLM-L6-v2/tree/main) and ran that within the service.  \n",
       "8896e9be994a49f8                                                                                                                                                                                                                                                                                                                                                                                                                           preview_image: /benchmarks/benchmark-1.png\\n\\nseo_schema: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"headline\": \"Vector Search Comparative Benchmarks\", \"image\": [ \"https://qdrant.tech/benchmarks/benchmark-1.png\" ], \"abstract\": \"The first comparative benchmark and benchmarking framework for vector search engines\", \"datePublished\": \"2022-08-23\", \"dateModified\": \"2022-08-23\", \"author\": [{ \"@type\": \"Organization\", \"name\": \"Qdrant\", \"url\": \"https://qdrant.tech\" }] }\\n\\n \\n\\n---\\n\\n. In this article, we will compare how Qdrant performs against the other vector search engines.  \n",
       "8969248e1b4c6205                                                                                                                                                                                                                                                               2. Vector search with keyword-based search. This one is covered in this article.\\n\\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\\n\\n\\n\\n## Why do we still need keyword search?\\n\\n\\n\\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\\n\\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\\n\\n. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.  \n",
       "9fe19aa1d622b9a6  compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\\n\\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\\n\\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\\n\\nmore effectively. ‚ÄúThis allowed us to scale smoothly from there,‚Äù Polu says.\\n\\n![‚ÄúWe were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\\n\\nwe don‚Äôt have to run lots of nodes in parallel. While being memory-bound, we were\\n\\nable to push the same instances further with the help of quantization. While you\\n\\nget pressure on MMAP in this case you maintain very good performance even if the\\n\\nRAM is fully used. With this we were able to reduce our cost by 2x.‚Äù - Stanislas Polu, Co-Founder of Dust](/case-st...  \n",
       "7838606c9ae3bc39  ---\\n\\ntitle: Langchain Go\\n\\nweight: 120\\n\\n---\\n\\n\\n\\n# Langchain Go\\n\\n\\n\\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\\n\\n\\n\\nYou can use Qdrant as a vector store in Langchain Go.\\n\\n\\n\\n## Setup\\n\\n\\n\\nInstall the `langchain-go` project dependency\\n\\n\\n\\n```bash\\n\\ngo get -u github.com/tmc/langchaingo\\n\\n```\\n\\n\\n\\n## Usage\\n\\n\\n\\nBefore you use the following code sample, customize the following values for your configuration:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\n\\n\\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \\n\\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\\n\\n\\n\\n```python\\n\\nfrom langchain.vectorstores import Qdrant\\n\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\nembeddings = HuggingFaceEmbeddings(\\n\\n    model...  \n",
       "2869ccb19423efe8                                                                                                                                                                       In this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n**What is neural search?**\\n\\nFrom web-pages search to product recommendations.\\n\\nFor many years, this technology didn't get much change until neural networks came into play.\\n\\n\\n\\nIn this tutorial we are going to find answers to these questions:\\n\\n\\n\\n* What is the difference between regular and neural search?\\n\\n* What neural networks could be used for search?\\n\\n* In what tasks is neural network search useful?\\n\\n* How to build and deploy own neural search service step-by-step?\\n\\n\\n\\n## What is neural search?  \n",
       "e24aea5dff264cce                                                                                                                                  On top of it, there is also a problem with search accuracy.\\n\\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\\n\\n\\n\\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.\\n\\nRead more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.\\n\\nHNSW is chosen for several reasons.\\n\\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\\n\\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\\n\\n\\n\\n*Available as of v1.1.1*\\n\\n\\n\\nThe HNSW parameters can also be configured on a collection and named vector\\n\\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\\n\\nperformance.  \n",
       "99311e9c03c71f6d                                                                                                                                                                                   ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nYeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user  \n",
       "b5fa4a67b747eca0  ---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.\\n\\nQuantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search alg...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">acac1350b30419a7</th>\n",
       "      <th>0</th>\n",
       "      <td>d696bfe6c1d5c59504a3d36db03cebfa</td>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>{{&lt; figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" &gt;}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.</td>\n",
       "      <td>0.884858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d696bfe6c1d5c59504a3d36db03cebfa</td>\n",
       "      <td>What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?</td>\n",
       "      <td>Diversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix</td>\n",
       "      <td>0.856284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1e618386a651dd3d</th>\n",
       "      <th>0</th>\n",
       "      <td>1ced6331fee751b540b0bc1b4185f566</td>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.881565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ced6331fee751b540b0bc1b4185f566</td>\n",
       "      <td>How can you parallelize the upload of a large dataset using shards in Qdrant?</td>\n",
       "      <td>setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is</td>\n",
       "      <td>0.796063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd4b03b790d138f7</th>\n",
       "      <th>0</th>\n",
       "      <td>0f79ae1cdb4939de42c2a640c80c1dd1</td>\n",
       "      <td>What models does Qdrant support for embedding generation?</td>\n",
       "      <td>. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.</td>\n",
       "      <td>0.845401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">b5fa4a67b747eca0</th>\n",
       "      <th>0</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.</td>\n",
       "      <td>0.858141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.</td>\n",
       "      <td>0.814146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.</td>\n",
       "      <td>0.810805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.789160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bce3b2cf798ea5f3f5c7568e4a25f08f</td>\n",
       "      <td>What is quantization?</td>\n",
       "      <td>Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.</td>\n",
       "      <td>0.770703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "acac1350b30419a7 0                  d696bfe6c1d5c59504a3d36db03cebfa   \n",
       "                 1                  d696bfe6c1d5c59504a3d36db03cebfa   \n",
       "1e618386a651dd3d 0                  1ced6331fee751b540b0bc1b4185f566   \n",
       "                 1                  1ced6331fee751b540b0bc1b4185f566   \n",
       "cd4b03b790d138f7 0                  0f79ae1cdb4939de42c2a640c80c1dd1   \n",
       "...                                                              ...   \n",
       "b5fa4a67b747eca0 0                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 1                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 2                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 3                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "                 4                  bce3b2cf798ea5f3f5c7568e4a25f08f   \n",
       "\n",
       "                                                                                                                                                                            input  \\\n",
       "context.span_id  document_position                                                                                                                                                  \n",
       "acac1350b30419a7 0                  What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "                 1                  What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?   \n",
       "1e618386a651dd3d 0                                                                                  How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "                 1                                                                                  How can you parallelize the upload of a large dataset using shards in Qdrant?   \n",
       "cd4b03b790d138f7 0                                                                                                      What models does Qdrant support for embedding generation?   \n",
       "...                                                                                                                                                                           ...   \n",
       "b5fa4a67b747eca0 0                                                                                                                                          What is quantization?   \n",
       "                 1                                                                                                                                          What is quantization?   \n",
       "                 2                                                                                                                                          What is quantization?   \n",
       "                 3                                                                                                                                          What is quantization?   \n",
       "                 4                                                                                                                                          What is quantization?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "acac1350b30419a7 0                                                                      {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=\"Example of similarity-based sampling\" >}}\\n\\n\\n\\n\\n\\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\\n\\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.   \n",
       "                 1                  Diversity search is a method for finding the most distinctive examples in the data.\\n\\nAs similarity search, it also operates on embeddings and measures the distances between them.\\n\\nThe difference lies in deciding which point should be extracted next.\\n\\n\\n\\nLet's imagine how to get 3 points with similarity search and then with diversity search.\\n\\n\\n\\nSimilarity:\\n\\n1. Calculate distance matrix\\n\\n2. Choose your anchor\\n\\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix   \n",
       "1e618386a651dd3d 0                     ## Parallel upload into multiple shards\\n\\n\\n\\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\\n\\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n      \"size\": 768,\\n\\n      \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 2\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "                 1                                                                                     setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \\n\\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \\n\\nlarge amounts of data without sacrificing performance or reliability.\\n\\n\\n\\n## Administration API\\n\\n\\n\\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is   \n",
       "cd4b03b790d138f7 0                                                                                                            . So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n",
       "b5fa4a67b747eca0 0                                        ---\\n\\ntitle: Quantization\\n\\nweight: 120\\n\\naliases:\\n\\n  - ../quantization\\n\\n---\\n\\n\\n\\n# Quantization\\n\\n\\n\\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\\n\\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\\n\\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.   \n",
       "                 1                                              Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.   \n",
       "                 2                                                      Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\\n\\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\\n\\n\\n\\nThere are tradeoffs associated with quantization.\\n\\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.   \n",
       "                 3                                                          *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "                 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Check out our [Quantum Quantization PR](https://github.com/qdrant/qdrant/pull/1639) on GitHub.   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "acac1350b30419a7 0                        0.884858  \n",
       "                 1                        0.856284  \n",
       "1e618386a651dd3d 0                        0.881565  \n",
       "                 1                        0.796063  \n",
       "cd4b03b790d138f7 0                        0.845401  \n",
       "...                                            ...  \n",
       "b5fa4a67b747eca0 0                        0.858141  \n",
       "                 1                        0.814146  \n",
       "                 2                        0.810805  \n",
       "                 3                        0.789160  \n",
       "                 4                        0.770703  \n",
       "\n",
       "[74 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. Define your evaluation model and your evaluators for Hybrid Search**\n",
    "\n",
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf53fa9a415460e8a30f2eec70c17b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/56 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c46e25d130d449dad332edd3478d38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/74 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_pyarrow_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m hallucination_eval_df_hybrid, qa_correctness_eval_df_hybrid \u001b[38;5;241m=\u001b[39m run_evals(\n\u001b[1;32m     15\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mqueries_df_hybrid,\n\u001b[1;32m     16\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39m[hallucination_evaluator, qa_correctness_evaluator],\n\u001b[1;32m     17\u001b[0m     provide_explanation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m relevance_eval_df_hybrid \u001b[38;5;241m=\u001b[39m run_evals(\n\u001b[1;32m     20\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mretrieved_documents_df_hybrid,\n\u001b[1;32m     21\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39m[relevance_evaluator],\n\u001b[1;32m     22\u001b[0m     provide_explanation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m \u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_evaluations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSpanEvaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHallucination\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhallucination_eval_df_hybrid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSpanEvaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQA Correctness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_correctness_eval_df_hybrid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mHYBRID_RAG_PROJECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m px\u001b[38;5;241m.\u001b[39mClient()\u001b[38;5;241m.\u001b[39mlog_evaluations(DocumentEvaluations(eval_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mrelevance_eval_df_hybrid),\n\u001b[1;32m     31\u001b[0m                             HYBRID_RAG_PROJECT)\n",
      "File \u001b[0;32m~/qdrant/workspace/qdrant-rag-eval/workshop-rag-eval-qdrant-arize/arize-eval/lib/python3.10/site-packages/phoenix/session/client.py:183\u001b[0m, in \u001b[0;36mClient.log_evaluations\u001b[0;34m(self, project_name, *evals)\u001b[0m\n\u001b[1;32m    181\u001b[0m project_name \u001b[38;5;241m=\u001b[39m project_name \u001b[38;5;129;01mor\u001b[39;00m get_env_project_name()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evaluation \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[0;32m--> 183\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pyarrow_table\u001b[49m()\n\u001b[1;32m    184\u001b[0m     sink \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mBufferOutputStream()\n\u001b[1;32m    185\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/x-pandas-arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_pyarrow_table'"
     ]
    }
   ],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "# all spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "    eval_model = OpenAIModel(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "    )\n",
    "    hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "    qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "    relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "    \n",
    "    hallucination_eval_df_hybrid, qa_correctness_eval_df_hybrid = run_evals(\n",
    "        dataframe=queries_df_hybrid,\n",
    "        evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "    relevance_eval_df_hybrid = run_evals(\n",
    "        dataframe=retrieved_documents_df_hybrid,\n",
    "        evaluators=[relevance_evaluator],\n",
    "        provide_explanation=True,\n",
    "    )[0]\n",
    "    \n",
    "    px.Client().log_evaluations(\n",
    "        SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df_hybrid),\n",
    "        SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df_hybrid),\n",
    "        HYBRID_RAG_PROJECT,\n",
    "    )\n",
    "    px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df_hybrid),\n",
    "                                HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Your evaluations should now appear as annotations on the appropriate spans in Phoenix.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003c1412566441b2a0fd878aabed07a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01d4ff90aac943ff83bfe49b70c783ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d9844dfef3f414aa750e6200a62bfd3",
       "IPY_MODEL_57d8cc8ff7f047c19301f0793367b2ae",
       "IPY_MODEL_4b23a2eb73b6435384686ced47f9c7f6"
      ],
      "layout": "IPY_MODEL_6fc5bf4070d94eccbdcb4fad4247c758"
     }
    },
    "02ca3a5d43e343d5b756fca172bc1822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1e6788ea4344553adbe66202caa656c",
       "IPY_MODEL_5a3d5d83788446028f4de4afbd262f3e",
       "IPY_MODEL_bf06bcbdbff3485abb3895dc54256e00"
      ],
      "layout": "IPY_MODEL_1ff9ec98314944bdb86d4fda42a4c88b"
     }
    },
    "0685ef29e4c54e2e8d5c424508ebfe82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "080a6d0c05144698a9c7366e54bb39b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f86e93b0fe48ddbaf2cabc9975c68b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8e261a6071cd41a6afd4377c9ead98ca",
      "value": "‚Äá35/35‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:37&lt;00:00‚Äá|‚Äá‚Äá1.72it/s"
     }
    },
    "0a8c4c23041641308b625e929790a55e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c90d55a6f6d4d3f8c72bd0ba19687ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be68eca0c7b640abada072a3cc3cd042",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6eadc3109208449eb9cb7ad5b6a5bb46",
      "value": "‚Äá125k/125k‚Äá[00:01&lt;00:00,‚Äá84.2kB/s]"
     }
    },
    "0d229e81fcb246cf99dd919d06cf5905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1057e11184c44ea38154a2ceabd90198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4722676ae36d4749918f5edaac48b182",
       "IPY_MODEL_a2041da6fb5f499191f405fa891c0907",
       "IPY_MODEL_5c26103d1d89466c9d22960bb347db39"
      ],
      "layout": "IPY_MODEL_0685ef29e4c54e2e8d5c424508ebfe82"
     }
    },
    "10d5d30b4fd14f3ba45da1794607c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "128b89e88c684401818304e8410b6c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1677f41c03a74a9f8ac55c49a9c12799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bda9ff3e5071497c933f31a2dd44b449",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b6686121f3084e1a8c6e0399a8a72675",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "178bbf06a9ae4b52baa5b731c0c81590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003c1412566441b2a0fd878aabed07a6",
      "max": 4431,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf1c5baa009c467a8c76a1a78d6987b1",
      "value": 4431
     }
    },
    "18ee1349c9394dab81163d5de23a04a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ad21fe66fac4742beb7c17ba8f8733f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1677f41c03a74a9f8ac55c49a9c12799",
       "IPY_MODEL_5b8040a4238f41128006185a858db466",
       "IPY_MODEL_7fbfaf28e7134c638b3e4d16e3cb3e6c"
      ],
      "layout": "IPY_MODEL_1f1e8f2a35b645eb91e96a847cf2856b"
     }
    },
    "1e846da2f0c44e099d24c350e0b0eb1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f1e8f2a35b645eb91e96a847cf2856b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3117c63e8d4558b801a33e9b5c548a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff9ec98314944bdb86d4fda42a4c88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218ea3901010405a95033a6cb632250a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e41bb05ed34b02b2e9e1c2d7d56145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c94d8ddc9d4561930044518630bb0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ee1349c9394dab81163d5de23a04a8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_21e41bb05ed34b02b2e9e1c2d7d56145",
      "value": "‚Äá4431/4431‚Äá[00:02&lt;00:00,‚Äá2155.10it/s]"
     }
    },
    "251b1e790043469393aae0c8ebec3d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280b4a0048134a8aaf523deead4e3aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "366e10ebb46a45508d837c8ead93124c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a222b53b0b2448a97b23e2b782a37b5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99f0a6ea567247639e7b17af1e1df9cf",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "37a8add4c6a242a49ea09ebef29cfdd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993aeba0d0f4021b4ae31b8378af903",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfbcd04f19374fe6b8ab9802f6891612",
      "value": "‚Äá240/0‚Äá[00:00&lt;00:00,‚Äá2042.22‚Äáexamples/s]"
     }
    },
    "38d701e482684695b97bd73bac78ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43dfd4070e324858974c08fbcb8055fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43e63c6009c14d9cb4eee3ea177d1c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f997d5f3659421981252a05845dd236",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_af7527216b9f4240b36024d07ac5c507",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá946B/s]"
     }
    },
    "44cb1b5c1c0a446f9d308d93d33ae4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50d006eafd294bbab3339c2bece90673",
      "max": 335,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21c626648d643d4972c76a094391134",
      "value": 335
     }
    },
    "4722676ae36d4749918f5edaac48b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3888c2a5954fb29293f41577a5bcae",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f7b5096a4b944d62a281ee927a1ce2e4",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "4ada07ba27b14c1a8c071343cc21c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b23a2eb73b6435384686ced47f9c7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9fa5a2efb1142c9abd47fe46f05cee6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_56dc39917eec4eba8194bbe0a3e91de1",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá1.71kB/s]"
     }
    },
    "4cc3f02b7bb44a87b8d50dd2f98edce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "503e9200b403425b8c0379b0e74b01fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50d006eafd294bbab3339c2bece90673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54aa6aff06214fe4baa3e621ec306c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "56dc39917eec4eba8194bbe0a3e91de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d8cc8ff7f047c19301f0793367b2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ce212a2baa494a8a9ae439c7c3e742",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f61b254bd9e24480b0176b43c1c7f47e",
      "value": 43
     }
    },
    "5a3d5d83788446028f4de4afbd262f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc3f02b7bb44a87b8d50dd2f98edce7",
      "max": 1777260,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e07726709c7748eeb708dbe5c37f9770",
      "value": 1777260
     }
    },
    "5b8040a4238f41128006185a858db466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_503e9200b403425b8c0379b0e74b01fe",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f6e8b35568945dc85ba3e47f7b0f3d7",
      "value": 2048
     }
    },
    "5c26103d1d89466c9d22960bb347db39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ada07ba27b14c1a8c071343cc21c9c4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_98d7cd89900f44a79893e7d8d6b8b0c7",
      "value": "‚Äá81/0‚Äá[00:00&lt;00:00,‚Äá1674.70‚Äáexamples/s]"
     }
    },
    "5c65981c04f34589a36c5c8bb4195f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cbb12c59ec948ba8d66dc3812d5c846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602df92e53334c65a2cc171301c67e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65f86e93b0fe48ddbaf2cabc9975c68b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66588b1f23bb421db7dbc90b2298a845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68ac22f645934827a9627e3559067ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3c5e97c2ac94cafa20b130f1f45ce67",
       "IPY_MODEL_b505d5e7978d4b19b59dc8f3fe71e795",
       "IPY_MODEL_080a6d0c05144698a9c7366e54bb39b0"
      ],
      "layout": "IPY_MODEL_e69da775e4df4ed8a5f5ed1348278ab8"
     }
    },
    "6a222b53b0b2448a97b23e2b782a37b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b115bdeaeb547e6b96a5a440b2fe3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eadc3109208449eb9cb7ad5b6a5bb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f6e8b35568945dc85ba3e47f7b0f3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fc5bf4070d94eccbdcb4fad4247c758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ff26418624f4662871a69796f67427a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72ce212a2baa494a8a9ae439c7c3e742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d79ac84c8c4d8393fcdb7f304b4bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79335cc0bf85458ba630ced71d706b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_251b1e790043469393aae0c8ebec3d77",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_602df92e53334c65a2cc171301c67e46",
      "value": 43
     }
    },
    "7b8b3e1c571a4345bc2ec4eb00f0967e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a74d6dce715f4494874de92852b56c09",
       "IPY_MODEL_a5d80d3579ca481a98310bd08b8c6918",
       "IPY_MODEL_f07a59b5bc7a4f56baa892ca864d488c"
      ],
      "layout": "IPY_MODEL_84fd76a737334831a886755affb50886"
     }
    },
    "7e8c241b51af4963aa78d91e6e78136e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fbfaf28e7134c638b3e4d16e3cb3e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c47e7205eb6540a8a5c51120cd222fe3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ea74a3a2d06a4c61b85db3c7552535a1",
      "value": "‚Äá2048/2048‚Äá[00:20&lt;00:00,‚Äá120.27it/s]"
     }
    },
    "8115b0b37d164080b58368be3fadbaa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84fd76a737334831a886755affb50886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869c8dd95cf7452b841f7203e0e6b8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9844dfef3f414aa750e6200a62bfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218ea3901010405a95033a6cb632250a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8ef997a7dbf54d3d9cfa8147e1611cbd",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "8e261a6071cd41a6afd4377c9ead98ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ef997a7dbf54d3d9cfa8147e1611cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f997d5f3659421981252a05845dd236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90f264886e864ddabdb6eddbecf6d75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "924c8fe0a9cb4feab39d31ec358089b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98d7cd89900f44a79893e7d8d6b8b0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "997305a8d871401ba6a1e805dc1ca045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99f0a6ea567247639e7b17af1e1df9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f88771edbfd468cb315a54b69eb7226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c3867b1d404a948a0edd0e6bc9a1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff9f4b2c32234207acb5221babe9c061",
      "max": 124978,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51b6ff6e830489f8e1e100bf1f1ae98",
      "value": 124978
     }
    },
    "a2041da6fb5f499191f405fa891c0907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54aa6aff06214fe4baa3e621ec306c04",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66588b1f23bb421db7dbc90b2298a845",
      "value": 1
     }
    },
    "a5d80d3579ca481a98310bd08b8c6918": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce0efd7352cd4539bda8706ebbc146fd",
      "max": 22,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128b89e88c684401818304e8410b6c33",
      "value": 22
     }
    },
    "a74d6dce715f4494874de92852b56c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c65981c04f34589a36c5c8bb4195f2b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_10d5d30b4fd14f3ba45da1794607c5e0",
      "value": "run_evals‚Äá"
     }
    },
    "aa8db2c8310b4f9582f2002adf3ffa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b84c10312dbe486fb3b21c7c5e23d603",
       "IPY_MODEL_178bbf06a9ae4b52baa5b731c0c81590",
       "IPY_MODEL_22c94d8ddc9d4561930044518630bb0b"
      ],
      "layout": "IPY_MODEL_acf3351a0fba41558f97945fbc6b84c7"
     }
    },
    "ac507e1d00b446df9379fa58f39418fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85db136dc2843a58e7aa648bbf9f9fe",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5cbb12c59ec948ba8d66dc3812d5c846",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "acf3351a0fba41558f97945fbc6b84c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adc2166e9b7d4ebdb5dd5415fd3e2ec5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "af7527216b9f4240b36024d07ac5c507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af77bbc36b844afa960381389111bccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7d2403f4504d49a027d935b13d3f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ff26418624f4662871a69796f67427a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5e93af9e5644c419bc653af2057e285",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "b21c626648d643d4972c76a094391134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3dc75c3ec7a48ceaff045c81f250bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b44f529954e944729b7446a4d610ac1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b505d5e7978d4b19b59dc8f3fe71e795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38d701e482684695b97bd73bac78ccfc",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7d21aa7bce145b78e834897b158f9b0",
      "value": 35
     }
    },
    "b51b292bc0b341de8fd94707e7fa2c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8749c8627c44757aa8703b45421af2d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9f88771edbfd468cb315a54b69eb7226",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "b6686121f3084e1a8c6e0399a8a72675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b84c10312dbe486fb3b21c7c5e23d603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f3117c63e8d4558b801a33e9b5c548a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_997305a8d871401ba6a1e805dc1ca045",
      "value": "Parsing‚Äánodes:‚Äá100%"
     }
    },
    "b993aeba0d0f4021b4ae31b8378af903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3888c2a5954fb29293f41577a5bcae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda9ff3e5071497c933f31a2dd44b449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be68eca0c7b640abada072a3cc3cd042": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf06bcbdbff3485abb3895dc54256e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8115b0b37d164080b58368be3fadbaa0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d229e81fcb246cf99dd919d06cf5905",
      "value": "‚Äá1.78M/1.78M‚Äá[00:00&lt;00:00,‚Äá4.56MB/s]"
     }
    },
    "c30b902e82de486eb0c10a3e64687b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a80fdcb9944e62ba5b51838fef5ef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af7d2403f4504d49a027d935b13d3f62",
       "IPY_MODEL_79335cc0bf85458ba630ced71d706b2e",
       "IPY_MODEL_43e63c6009c14d9cb4eee3ea177d1c7d"
      ],
      "layout": "IPY_MODEL_dce488c3561e4d7fa737379553ec0b3b"
     }
    },
    "c47e7205eb6540a8a5c51120cd222fe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c762c26d4f2e4159b714ad55bbd195eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b51b292bc0b341de8fd94707e7fa2c0c",
       "IPY_MODEL_f10f2f679d7a4ab8988601cd59aa4b70",
       "IPY_MODEL_d5baee84346c4dc29ffa4fd2fe6495c6"
      ],
      "layout": "IPY_MODEL_0a8c4c23041641308b625e929790a55e"
     }
    },
    "c85db136dc2843a58e7aa648bbf9f9fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95d4cfffece443182faa3e2a3e48dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df997aad597e4c1ab23501d12383e39b",
       "IPY_MODEL_a1c3867b1d404a948a0edd0e6bc9a1b8",
       "IPY_MODEL_0c90d55a6f6d4d3f8c72bd0ba19687ba"
      ],
      "layout": "IPY_MODEL_1e846da2f0c44e099d24c350e0b0eb1f"
     }
    },
    "ce0efd7352cd4539bda8706ebbc146fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1c5baa009c467a8c76a1a78d6987b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfbcd04f19374fe6b8ab9802f6891612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cff10cfd18704a9cb7c9b4d4ba5ccaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d44a32f577204732a7f4ce767d8e83f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d51b6ff6e830489f8e1e100bf1f1ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5baee84346c4dc29ffa4fd2fe6495c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_280b4a0048134a8aaf523deead4e3aef",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_869c8dd95cf7452b841f7203e0e6b8c5",
      "value": "‚Äá2048/2048‚Äá[00:19&lt;00:00,‚Äá119.05it/s]"
     }
    },
    "d5e93af9e5644c419bc653af2057e285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d772ecb46ca54adbbb84749fe07ab735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8749c8627c44757aa8703b45421af2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9fa5a2efb1142c9abd47fe46f05cee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce488c3561e4d7fa737379553ec0b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df997aad597e4c1ab23501d12383e39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_924c8fe0a9cb4feab39d31ec358089b8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "e07726709c7748eeb708dbe5c37f9770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e69da775e4df4ed8a5f5ed1348278ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7d21aa7bce145b78e834897b158f9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7d4e009f37a4d798dbc23b39ed7c0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea74a3a2d06a4c61b85db3c7552535a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb848aa889474b118c49a7cdac509d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f264886e864ddabdb6eddbecf6d75a",
      "value": 1
     }
    },
    "eecec2323e294d6fb4ee89f5937c241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac507e1d00b446df9379fa58f39418fc",
       "IPY_MODEL_eb848aa889474b118c49a7cdac509d9b",
       "IPY_MODEL_37a8add4c6a242a49ea09ebef29cfdd9"
      ],
      "layout": "IPY_MODEL_b44f529954e944729b7446a4d610ac1d"
     }
    },
    "f07a59b5bc7a4f56baa892ca864d488c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d4e009f37a4d798dbc23b39ed7c0b1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d772ecb46ca54adbbb84749fe07ab735",
      "value": "‚Äá22/22‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:52&lt;00:00‚Äá|‚Äá‚Äá2.31it/s"
     }
    },
    "f10f2f679d7a4ab8988601cd59aa4b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af77bbc36b844afa960381389111bccf",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3dc75c3ec7a48ceaff045c81f250bfd",
      "value": 2048
     }
    },
    "f1e6788ea4344553adbe66202caa656c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e47bbf438740a2a224c5f7d913e502",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c30b902e82de486eb0c10a3e64687b44",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "f3c5e97c2ac94cafa20b130f1f45ce67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77d79ac84c8c4d8393fcdb7f304b4bf3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_43dfd4070e324858974c08fbcb8055fd",
      "value": "run_evals‚Äá"
     }
    },
    "f61b254bd9e24480b0176b43c1c7f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7b5096a4b944d62a281ee927a1ce2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e47bbf438740a2a224c5f7d913e502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa26918ac71440c1803c82c939fbc79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_366e10ebb46a45508d837c8ead93124c",
       "IPY_MODEL_44cb1b5c1c0a446f9d308d93d33ae4f8",
       "IPY_MODEL_fca056b08fc3427a977bc3717a020443"
      ],
      "layout": "IPY_MODEL_7e8c241b51af4963aa78d91e6e78136e"
     }
    },
    "fca056b08fc3427a977bc3717a020443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44a32f577204732a7f4ce767d8e83f5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6b115bdeaeb547e6b96a5a440b2fe3ef",
      "value": "‚Äá335/335‚Äá[00:05&lt;00:00,‚Äá59.78it/s]"
     }
    },
    "ff9f4b2c32234207acb5221babe9c061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
